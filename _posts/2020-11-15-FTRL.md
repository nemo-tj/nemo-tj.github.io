---
layout: post
title: æœºå™¨å­¦ä¹ :FTRL 
modified: 2020-11-15
tags: [online learning, machine learning, ftrl]
---
## åœ¨çº¿å­¦ä¹ ä¸­ä¸¤ä¸ªé—®é¢˜{Online Learning: Regret + Sparsity} 
åœ¨çº¿å­¦ä¹  ( `Online Learning`  ) ä»£è¡¨äº†ä¸€ç³»åˆ—æœºå™¨å­¦ä¹ ç®—æ³•ï¼Œç‰¹ç‚¹æ˜¯æ¯æ¥ä¸€ä¸ªæ ·æœ¬å°±èƒ½è®­ç»ƒï¼Œèƒ½å¤Ÿæ ¹æ®çº¿ä¸Šåé¦ˆæ•°æ®ï¼Œå®æ—¶å¿«é€Ÿåœ°è¿›è¡Œæ¨¡å‹è°ƒæ•´ï¼Œä½¿å¾—æ¨¡å‹åŠæ—¶åæ˜ çº¿ä¸Šçš„å˜åŒ–ï¼Œæé«˜çº¿ä¸Šé¢„æµ‹çš„å‡†ç¡®ç‡ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œä¼ ç»Ÿçš„æ‰¹å¤„ç†æ–¹å¼éœ€è¦ä¸€æ¬¡æ€§æ”¶é›†æ‰€æœ‰æ•°æ®ï¼Œæ–°æ•°æ®åˆ°æ¥æ—¶é‡æ–°è®­ç»ƒçš„ä»£ä»·ä¹Ÿå¾ˆå¤§ï¼Œå› è€Œæ›´æ–°å‘¨æœŸè¾ƒé•¿ï¼Œå¯æ‰©å±•æ€§ä¸é«˜ã€‚

ä¸€èˆ¬å¯¹äºåœ¨çº¿å­¦ä¹ æ¥è¯´ï¼Œæˆ‘ä»¬è‡´åŠ›äºè§£å†³ä¸¤ä¸ªé—®é¢˜ï¼š é™ä½ `regret` å’Œæé«˜ `sparsity`ã€‚å…¶ä¸­ regret çš„å®šä¹‰ä¸º:

$$
\begin{equation}
    Regret = \sum\limits_{t=1}^T \ell_t({\bf{w_t}}) - \min_\bf{w}\sum\limits_{t=1}^T\ell_t(\bf{w}) 
\end{equation}
$$
å…¶ä¸­ $ğ‘¡$ è¡¨ç¤ºæ€»å…± $ğ‘‡$ è½®ä¸­çš„ç¬¬ $ğ‘¡$ è½®è¿­ä»£ï¼Œ$\ell_t$ è¡¨ç¤ºæŸå¤±å‡½æ•°ï¼Œ$ğ°$ è¡¨ç¤ºè¦å­¦ä¹ çš„å‚æ•°ã€‚ç¬¬äºŒé¡¹ $\min\limits_\bf{w}\sum\limits_{t=1}^T\ell_t(\bf{w})$ è¡¨ç¤ºå¾—åˆ°äº†æ‰€æœ‰æ ·æœ¬åæŸå¤±å‡½æ•°çš„æœ€ä¼˜è§£ï¼Œå› ä¸ºåœ¨çº¿å­¦ä¹ ä¸€æ¬¡åªèƒ½æ ¹æ®å°‘æ•°å‡ ä¸ªæ ·æœ¬æ›´æ–°å‚æ•°ï¼Œéšæœºæ€§è¾ƒå¤§ï¼Œæ‰€ä»¥éœ€è¦ä¸€ç§ç¨³å¥çš„ä¼˜åŒ–æ–¹å¼ï¼Œè€Œ `regret` å­—é¢æ„æ€æ˜¯ â€œåæ‚”åº¦â€ï¼Œæ„å³æ›´æ–°å®Œä¸åæ‚”ã€‚

åœ¨ç†è®ºä¸Šå¯ä»¥è¯æ˜ï¼Œå¦‚æœä¸€ä¸ªåœ¨çº¿å­¦ä¹ ç®—æ³•å¯ä»¥ä¿è¯å…¶ `regret` æ˜¯ $ğ‘¡$ çš„æ¬¡çº¿æ€§å‡½æ•°ï¼Œåˆ™ï¼š
$$
 \lim\limits_{t \to \infty}\frac{\text{Regret}(t)}{t} = 0
$$
é‚£ä¹ˆéšç€è®­ç»ƒæ ·æœ¬çš„å¢å¤šï¼Œåœ¨çº¿å­¦ä¹ å‡ºæ¥çš„æ¨¡å‹æ— é™æ¥è¿‘äºæœ€ä¼˜æ¨¡å‹ã€‚è€Œæ¯«ä¸æ„å¤–çš„ï¼Œ`FTRL` æ­£æ˜¯æ»¡è¶³è¿™ä¸€ç‰¹æ€§ã€‚

å¦ä¸€æ–¹é¢ï¼Œç°å®ä¸­å¯¹äº `sparsity` ï¼Œä¹Ÿå°±æ˜¯æ¨¡å‹çš„ç¨€ç–æ€§ä¹Ÿå¾ˆçœ‹ä¸­ã€‚ä¸Šäº¿çš„ç‰¹å¾å¹¶ä¸é²œè§ï¼Œæ¨¡å‹è¶Šå¤æ‚ï¼Œéœ€è¦çš„å­˜å‚¨ã€æ—¶é—´èµ„æºä¹Ÿéšä¹‹å‡é«˜ï¼Œè€Œç¨€ç–çš„æ¨¡å‹ä¼šå¤§å¤§å‡å°‘é¢„æµ‹æ—¶çš„å†…å­˜å’Œå¤æ‚åº¦ã€‚å¦å¤–ç¨€ç–çš„æ¨¡å‹ç›¸å¯¹å¯è§£é‡Šæ€§ä¹Ÿè¾ƒå¥½ï¼Œè¿™ä¹Ÿæ­£æ˜¯é€šå¸¸æ‰€è¯´çš„ `L1` æ­£åˆ™åŒ–çš„ä¼˜ç‚¹ã€‚

åæ–‡ä¸»è¦è€ƒå¯Ÿ `FTRL` æ˜¯å¦‚ä½•å®ç°é™ä½ `regret` å’Œæé«˜ `sparsity` è¿™ä¸¤ä¸ªç›®æ ‡çš„ã€‚

## ä¼˜åŒ–è½è„šç‚¹{Gradient Learning: Learning Rate + Regulation}
 `OGD` ( `online gradient descent` ) æ˜¯ä¼ ç»Ÿæ¢¯åº¦ä¸‹é™çš„ online ç‰ˆæœ¬ï¼Œå‚æ•°æ›´æ–°å…¬å¼ä¸ºï¼š

$$
\begin{equation}
\bf{w}_{t+1} = \bf{w}_t - \eta_t \bf{g}_t
\end{equation}
$$


`OGD` åœ¨å‡†ç¡®ç‡ä¸Šè¡¨ç°ä¸é”™ï¼Œå³ `regret` ä½ï¼Œä½†åœ¨ä¸Šæ–‡çš„å¦ä¸€ä¸ªè€ƒé‡å› ç´  `sparsity` ä¸Šåˆ™è¡¨ç°ä¸ä½³ï¼Œå³ä½¿åŠ ä¸Šäº† $L1$ æ­£åˆ™ä¹Ÿå¾ˆéš¾ä½¿å¤§é‡çš„å‚æ•°å˜é›¶ã€‚ä¸€ä¸ªåŸå› æ˜¯æµ®ç‚¹è¿ç®—å¾ˆéš¾è®©æœ€åçš„å‚æ•°å‡ºç°ç»å¯¹é›¶å€¼ï¼›å¦ä¸€ä¸ªåŸå› æ˜¯ä¸åŒäºæ‰¹å¤„ç†æ¨¡å¼ï¼Œ`online` åœºæ™¯ä¸‹æ¯æ¬¡ $ğ°$ çš„æ›´æ–°å¹¶ä¸æ˜¯æ²¿ç€å…¨å±€æ¢¯åº¦è¿›è¡Œä¸‹é™ï¼Œè€Œæ˜¯æ²¿ç€æŸä¸ªæ ·æœ¬çš„äº§ç”Ÿçš„æ¢¯åº¦æ–¹å‘è¿›è¡Œä¸‹é™ï¼Œæ•´ä¸ªå¯»ä¼˜è¿‡ç¨‹å˜å¾—åƒæ˜¯ä¸€ä¸ªâ€œéšæœºâ€ æŸ¥æ‰¾çš„è¿‡ç¨‹ï¼Œè¿™æ · `online` æœ€ä¼˜åŒ–æ±‚è§£å³ä½¿é‡‡ç”¨ $L1$ æ­£åˆ™åŒ–çš„æ–¹å¼ï¼Œ ä¹Ÿå¾ˆéš¾äº§ç”Ÿç¨€ç–è§£ã€‚æ­£å› ä¸º `OGD` å­˜åœ¨è¿™æ ·çš„é—®é¢˜ï¼ŒFTRL æ‰è‡´åŠ›äºåœ¨å‡†ç¡®ç‡ä¸é™ä½çš„å‰æä¸‹æé«˜ç¨€ç–æ€§ã€‚

å…¶å®ï¼ŒOGDçš„å…¬å¼ç­‰ä»·äº
$$
\bf{w}_{t+1} = \mathop{\text{argmin}}_\bf{w} \left(\bf{g}_t \cdot \bf{w} + \frac{1}{2\eta_t}||\bf{w} - \bf{w}_t||_2^2 \right) \tag{1.2}
$$
å¯¹è¯¥å¼ç›´æ¥æ±‚å¯¼å³å¯ï¼Œ$$ \bf{g}_t + \frac{1}{\eta_t}(\bf{w} - \bf{w}_t) = 0 \;\;\implies\;\; \bf{w} = \bf{w}_t - \eta_t \bf{g}_t $$ ã€‚æœ‰äº†è¿™ä¸ªå…¬å¼çš„åŸºç¡€ï¼ŒFTRLä¸»è¦ä»ä¸¤ä¸ªæ–¹é¢è¿›è¡Œä¼˜åŒ–
+ é™ä½ `regret` 
+ æé«˜ `sparsity` ã€‚

é¦–å…ˆï¼Œä¸ºäº†é™ä½ `regretï¼ŒFTRL` ç”¨ $ğ _{1:ğ‘¡}$ ä»£æ›¿ $g_t$ ï¼Œ$ğ _{1:ğ‘¡}$ ä¸ºå‰ $1$ åˆ° $t$ è½®æŸå¤±å‡½æ•°çš„ç´¯è®¡æ¢¯åº¦ï¼Œå³ 

$$
\bf{g}_{1:t} = \sum_{s=1}^t \bf{g}_s = \sum_{s=1}^t \nabla \ell_s(\bf{w_s})
$$
ç”±äºåœ¨çº¿å­¦ä¹ éšæœºæ€§å¤§çš„ç‰¹ç‚¹ï¼Œç´¯è®¡æ¢¯åº¦å¯é¿å…ç”±äºæŸäº›ç»´åº¦æ ·æœ¬å±€éƒ¨æŠ–åŠ¨å¤ªå¤§å¯¼è‡´é”™è¯¯åˆ¤æ–­ã€‚è¿™æ˜¯ä» `FTL ( Follow the Leader )` é‚£å€Ÿé‰´è€Œæ¥çš„ï¼Œè€Œ FTRL çš„å…¨ç§°ä¸º `Follow the Regularized Leader` ï¼Œä»åå­—ä¸Šçœ‹å…¶å®å°±æ˜¯åœ¨ `FTL` çš„åŸºç¡€ä¸ŠåŠ ä¸Šäº†æ­£åˆ™åŒ–é¡¹ï¼Œå³ (1.2) å¼ä¸­çš„  

$$
||\bf{w} - \bf{w_t}||_2^2
$$
è¿™æ„å‘³ç€æ¯æ¬¡æ›´æ–°æ—¶æˆ‘ä»¬ä¸å¸Œæœ›æ–°çš„ $ğ°$ ç¦»ä¹‹å‰çš„ $ğ°_ğ‘¡$ å¤ªè¿œ (è¿™ä¹Ÿæ˜¯æœ‰æ—¶å…¶è¢«ç§°ä¸º `FTRL-proximal` çš„åŸå› )ï¼Œè¿™åŒæ ·æ˜¯ä¸ºäº†é™ä½regretï¼Œåœ¨çº¿å­¦ä¹ å™ªéŸ³å¤§ï¼Œè‹¥ä¸€æ¬¡æ›´æ–°é”™å¾—å¤ªè¿œåé¢éš¾ä»¥æ”¶å›æ¥ï¼Œæ²¡æ³•è½»æ˜“â€œåæ‚”â€ã€‚

äºæ˜¯`FTRL`çš„å…¬å¼åœ¨`OGD`çš„åŸºç¡€ä¸Šè¢«æ”¹é€ æˆè¿™ä¸ªæ ·å­:

$$
\begin{equation}\begin{split}
\bf{w}_{t+1}&=\mathop{\text{argmin}}\limits_{\bf{w}}\left(\bf{g}_{1:t} \cdot \bf{w} + \frac12 \sum\limits_{s=1}^t \sigma_s ||\bf{w} - \bf{w}_s||_2^2 + \lambda_1||\bf{w}||_1 + \frac12 \lambda_2||\bf{w}||_2^2\right) \\

&=\mathop{\text{argmin}}_{\bf{w}} \left\{ \left(\bf{g}_{1:t} - \sum\limits_{s=1}^t\sigma_s\bf{w}_s \right) \cdot \bf{w} + \lambda_1||\bf{w}||_1 + \frac12 \left(\lambda_2 + \sum\limits_{s=1}^t \sigma_s \right) \cdot ||\bf{w}||_2^2 + \frac12 \sum\limits_{s=1}^t \sigma_s||\bf{w}_s||_2^2 \right\} 
\end{split}\tag{1.3}\end{equation}
$$

ç”±äº 
$$
\frac{1}{2} \sum\limits_{s=1}^t \sigma_s || \bf{w_s} ||_2^2
$$
ç›¸å¯¹äºè¦ä¼˜åŒ–çš„å˜é‡$\bf{w}$æ˜¯ä¸€ä¸ªå¸¸æ•°ï¼Œå¯ä»¥æ¶ˆå»è¿™ä¸€é¡¹ã€‚ä»¤ $$\bf{z}_t = \bf{g}_{1:t} - \sum\limits_{s=1}^t \sigma_s\bf{w}_s$$, ç®€åŒ–å¾—åˆ°
$$
\bf{w}_{t+1} = \mathop{\text{argmin}}_{\bf{w}} \left\{ \bf{z}_t \cdot \bf{w} + \lambda_1||\bf{w}||_1 + \frac12 \left(\lambda_2 + \sum\limits_{s=1}^t \sigma_s \right) \cdot ||\bf{w}||_2^2 \right\} 
$$

å°†ç‰¹å¾çš„å„ä¸ªç»´åº¦æ‹†å¼€æˆç‹¬ç«‹çš„æ ‡é‡æœ€å°åŒ–é—®é¢˜ï¼Œå¯¹äºç¬¬ $i$ ä¸ªç‰¹å¾ï¼Œå¯¹åº”çš„æƒé‡ä¸º

$$
w_{t+1,i} = \mathop{\text{argmin}}_{w_i \in \mathbb{R}} \left\{ z_{t,i} \, w + \lambda_1 |w_i| + \frac12 \left(\lambda_2 + \sum\limits_{s=1}^t \sigma_s \right) \cdot w_i^2 \right\} \tag{1.6}
$$
è¿™æ˜¯ä¸€ä¸ªæ— çº¦æŸçš„éå¹³æ»‘å‚æ•°ä¼˜åŒ–é—®é¢˜ï¼Œå…¶ä¸­ç¬¬äºŒé¡¹

$$
\lambda_1 |w_i|
$$


åœ¨ $$ğ‘¤_ğ‘–=0$$å¤„ä¸å¯å¯¼, å› è€Œå¸¸ç”¨çš„æ–¹æ³•æ˜¯ä½¿ç”¨æ¬¡å¯¼æ•°, è¿™é‡Œç›´æ¥ä¸Šç»“è®ºï¼š å®šä¹‰ 
$$\phi \in \partial |w_i^*|$$ä¸º $$|ğ‘¤_ğ‘–|$$ åœ¨ $$ğ‘¤^âˆ—_ğ‘–$$ å¤„çš„æ¬¡å¯¼æ•°ï¼Œäºæ˜¯æœ‰ï¼š
$$
\partial |w_i^*| =
\begin{cases}
\quad\quad\{1\} &\quad \text{if}\;\; w_i^* > 0 \\[1ex]
-1 < \phi < 1  & \quad \text{if}\;\; w_i^* = 0 \\[1ex] 
\quad\;\;\{-1\} &\quad \text{if}\;\; w_i^* < 0
\end{cases}
$$

ä»è€Œå¯¹ï¼ˆ1.6ï¼‰æ±‚å¯¼å¹¶ä»¤å…¶ä¸ºé›¶å¾—ï¼š

$$
\begin{equation}
    z_{t,i} + \lambda_1 \phi + \left(\lambda_2 + \sum\limits_{s=1}^t \sigma_s\right)\cdot w_i = 0  \tag{1.8}
\end{equation}
$$
åœ¨è¿™ä¸ªå…¬å¼ä¸­ï¼Œ$\lambda_1 > 0$ å’Œ $\left(\lambda_2 + \sum\limits_{s=1}^t \sigma_s\right) > 0$ï¼Œä¸ºäº†ä¿è¯(1.8)æˆç«‹ï¼Œå¯ä»¥å¾—åˆ°:
$$
 \begin{equation}
 w_{t+1,i} = 
\begin{cases}
\qquad\qquad \large{0}   & \text{if}\;\; |z_{t,i}| < \lambda_1 \\[2ex]
-\frac{1}{\lambda_2 + \sum_{s=1}^t\sigma_s} \left(z_{t,i} - \text{sgn}(z_{t,i})\cdot\lambda_1 \right) & \text{otherwise}  \tag{1.9}
\end{cases}
 \end{equation}
$$

å¯ä»¥çœ‹åˆ°å½“ $$\bf{z}_t = (\bf{g}_{1:t} - \sum\limits_{s=1}^t \sigma_s\bf{w}_s) <\lambda_1$$ æ—¶ï¼Œå‚æ•°ç½®ä¸ºé›¶ï¼Œè¿™å°±æ˜¯ FTRL ç¨€ç–æ€§çš„ç”±æ¥ã€‚å¦å¤–åŠ å…¥ L2 æ­£åˆ™å¹¶æ²¡æœ‰å½±å“æ¨¡å‹çš„ç¨€ç–æ€§ï¼Œä» (1.9) å¼çœ‹åªæ˜¯ä½¿å¾—åˆ†æ¯å˜å¤§ï¼Œè¿›è€Œ ğ‘¤ğ‘– æ›´è¶‹äºé›¶äº†ï¼Œè¿™åœ¨ç›´è§‰ä¸Šæ˜¯ç¬¦åˆæ­£åˆ™åŒ–æœ¬èº«çš„å®šä¹‰çš„.

è§‚å¯Ÿ (1.9) å¼è¿˜é—ç•™ä¸€ä¸ªé—®é¢˜ï¼Œ$\sigma_s$ çš„å€¼æ˜¯ä»€ä¹ˆå‘¢ï¼Ÿè¿™ç‰µæ¶‰åˆ° FTRL çš„å­¦ä¹ ç‡è®¾ç½®ã€‚å½“ç„¶ä¸¥æ ¼æ„ä¹‰ä¸Šçš„å­¦ä¹ ç‡æ˜¯ $\eta_t$ ï¼Œè€Œ $\sigma_t = \frac{1}{\eta_t} - \frac{1}{\eta_{t-1}}$ ï¼Œè®ºæ–‡ä¸­è¿™æ ·å®šä¹‰å¯èƒ½æ˜¯ä¸ºäº†æ¨å¯¼å’Œå®ç°çš„æ–¹ä¾¿ã€‚å‰æ–‡ (1.1) å¼ä¸­ OGD ä½¿ç”¨çš„æ˜¯ä¸€ä¸ªå…¨å±€å­¦ä¹ ç‡ $\eta_t = \frac{1}{\sqrt{t}}$ï¼Œä¼šéšç€è¿­ä»£è½®æ•°çš„å¢åŠ è€Œé€’å‡ï¼Œä½†è¯¥æ–¹æ³•çš„é—®é¢˜æ˜¯æ‰€æœ‰ç‰¹å¾ç»´åº¦éƒ½ä½¿ç”¨äº†ä¸€æ ·çš„å­¦ä¹ ç‡ã€‚

FTRL é‡‡ç”¨çš„æ˜¯ `Per-Coordinate Learning Rate`ï¼Œå³æ¯ä¸ªç‰¹å¾é‡‡ç”¨ä¸åŒçš„å­¦ä¹ ç‡ï¼Œè¿™ç§æ–¹æ³•è€ƒè™‘äº†è®­ç»ƒæ ·æœ¬æœ¬èº«åœ¨ä¸åŒç‰¹å¾ä¸Šåˆ†å¸ƒçš„ä¸å‡åŒ€æ€§ã€‚å¦‚æœä¸€ä¸ªç‰¹å¾å˜åŒ–å¿«ï¼Œåˆ™å¯¹åº”çš„å­¦ä¹ ç‡ä¹Ÿä¼šä¸‹é™å¾—å¿«ï¼Œåä¹‹äº¦ç„¶ã€‚å…¶å®è¿‘å¹´æ¥éšç€æ·±åº¦å­¦ä¹ çš„æµè¡Œè¿™ç§æ“ä½œå·²ç»æ˜¯å¾ˆå¸¸è§äº†ï¼Œå¸¸ç”¨çš„ AdaGradã€Adam ç­‰æ¢¯åº¦ä¸‹é™çš„å˜ç§éƒ½è•´å«ç€è¿™ç±»æ€æƒ³ã€‚FTRL ä¸­ç¬¬ $ğ‘¡$ è½®ç¬¬ $ğ‘–$ ä¸ªç‰¹å¾çš„å­¦ä¹ ç‡ä¸ºï¼š

$$
\begin{equation}
 \eta_{t, i} = \frac{\alpha}{\beta + \sqrt{\sum_{s=1}^t g_{s, i}^2}}  \tag{1.10}
 \end{equation}
$$
äºæ˜¯ 1.9 ä¸­$\sum_{s=1}^t \sigma_s$ å¾—è¿™ä¸€é¡¹ç­‰äºï¼š

$$
\begin{equation}\begin{split}
\sum\limits_{s=1}^t \sigma_s &= (\frac{1}{\eta_t} - \frac{1}{\eta_{t-1}}) + (\frac{1}{\eta_{t-1}} - \frac{1}{\eta_{t-2}}) + \cdots + (\frac{1}{\eta_1} - \frac{1}{\eta_0}) \\
&=\;\; \frac{1}{\eta_t} \;\;=\;\; \frac{\beta + \sqrt{\sum_{s=1}^tg_{s,i}^2}}{\alpha}
\end{split}\tag{1.11}\end{equation}
$$

ä»è€Œï¼Œ$w_{i+1}$çš„æ•°å­¦å…¬å¼ä¸Šçš„è¡¨ç¤ºä¸ºï¼š

$$
\begin{equation}
 w_{t+1,i} = 
\begin{cases}
\qquad\qquad \large{0}   & \quad\text{if}\;\; |z_{t,i}| < \lambda_1 \\[2ex]
- \left(\lambda_2 + \frac{\beta + \sqrt{\sum_{s=1}^t g_{s,i}^2}}{\alpha} \right)^{-1} \left(z_{t,i} - \text{sgn}(z_{t,i})\cdot\lambda_1 \right) & \quad \text{otherwise}  
\end{cases}\tag{1.12}
 \end{equation}
$$

## å·¥ç¨‹å®ç°ä¸Šçš„å…¬å¼è½¬æ¢

åœ¨ä»£ç å®ç°ä¸Šï¼Œè€ƒè™‘åˆ° $t$ åˆ° $t+1$ çš„è¿­ä»£çš„ç‰¹ç‚¹ï¼Œå¯ä»¥æŠŠä¸­é—´çš„ç»“æœå­˜èµ·æ¥ï¼Œæ–¹ä¾¿åç»­çš„è®¡ç®—:

$$
\begin{equation}\begin{split}
z_{t,i} &= g_{1:t, i} - \sum\limits_{s=1}^t \sigma_{s, i} w_{s,i} = \sum\limits_{s=1}^t g_{s,i} - \sum\limits_{s=1}^t \sigma_{s, i} w_{s,i} = z_{t-1,i} + (g_{t,i} - \sigma_{t,i}w_{t,i}) \\[1ex]
&= z_{t-1,i} + g_{t,i} - \left(\frac{1}{\eta_{t,i}} - \frac{1}{\eta_{t-1, i}} \right) w_{t,i} \\
&= z_{t-1,i} + g_{t,i} - \frac{\sqrt{\sum_{s=1}^t g_{s,i}^2} - \sqrt{\sum_{s=1}^{t-1} g_{s,i}^2}}{\alpha} w_{t,i}
\end{split} \tag{3.1}\end{equation}
$$

$$
\begin{equation}
 \begin{aligned}
 n_{t,i} &= \sum_{s=1}^t g_{s,i}^2 = \sum_{s=1}^{t-1} g_{s,i}^2 + g_{t,i}^2 \\
        &= n_{t-1,i} + g_{t,i}^2
 \end{aligned} 
 \tag{3.2}
\end{equation}
$$

$$
\begin{equation}
g_{t,i} = y_t (S(y_t f(\bf{x}_t)) - 1) x_i = y_t \left(\frac{1}{1 + e^{- y_t f(\bf{x}_t)}} - 1 \right) x_i  \tag{3.3}
\end{equation}
$$

## å‚è€ƒèµ„æ–™
[åœ¨çº¿ä¼˜åŒ–ç®—æ³• FTRL çš„åŸç†ä¸å®ç°](https://www.cnblogs.com/massquantity/p/12693314.html)
