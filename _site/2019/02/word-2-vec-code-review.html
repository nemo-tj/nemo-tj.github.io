<!DOCTYPE html>
<html lang="en-us">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <title>Wall-E</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="stylesheet" href="/assets/css/mobile.css">
    <link rel="stylesheet" href="/assets/css/vendor/syntax.css">
    <link rel="stylesheet" href="/assets/css/vendor/semantic.min.css"/>

    <link rel="shortcut icon" type="image/png" href="/assets/img/logo.png"/>
    <link href="https://fonts.googleapis.com/css?family=Open+Sans|Roboto" rel="stylesheet">
</head>

<body>
    
    <div class="page-wrap">
        <div class="header-wrapper">
    <header>
        <!-- logo and description -->
        
        <nav class="site-nav">
            <div class="left-nav">
                <!-- add titles to all buttons -->
                <div class="ui secondary menu inverted">
                    <a class="item" href="/">
                        Home
                    </a>
                    <a class="item" href="/about.html">
                        About
                    </a>
                    <a class="item" href="/archive.html">
                        Archive
                    </a>
                    <div class="right menu">
                        <div id="head-search" class="ui category search item">
                            <div class="ui transparent icon input">
                                <input placeholder="Search..." type="text" id="search-input" onkeypress="handleKeyPress()">
                                <i class="search link icon" onclick="handleSubmit()"></i>
                            </div>
                        </div>

                        <a class="item" href="http://stallman.org/facebook.html">
                            <i class="wechat link icon"></i>
                        </a>
                        <a class="item" href="https://github.com/nemo-tj">
                            <i class="github link icon"></i>
                        </a>
                        <a class="item" href="/feed.xml">Subscribe</a>
                    </div>
                </div>
            </div>
            <div class="right-nav"></div>
        </nav>
    </header>
</div>
        <main>
            <div class="post-container">
    <center>
        <span class="date">
            06 Feb 2019 
             
        </span>
        <h1 class="post-title">word2vec 源码解析</h1>
    </center>
    <div class="ui divider"></div>
    <h3 id="基本概念">基本概念</h3>
<h4 id="在wrod2vec工具中有如下的几个比较重要的概念">在wrod2vec工具中，有如下的几个比较重要的概念：</h4>
<ul>
  <li>CBOW</li>
  <li>Skip-Gram</li>
  <li>Hierarchical Softmax</li>
  <li>Negative Sampling</li>
</ul>

<p>其中CBOW和Skip-Gram是word2vec工具中使用到的两种不同的语言模型，而Hierarchical Softmax和Negative Sampling是对以上的两种模型的具体的优化方法。</p>

<h4 id="在word2vec工具中主要的工作包括">在word2vec工具中，主要的工作包括：</h4>

<ul>
  <li>预处理。即变量的声明，全局变量的定义等；</li>
  <li>构建词库。即包含文本的处理，以及是否需要有指定词库等；</li>
  <li>初始化网络结构。即包含CBOW模型和Skip-Gram模型的参数初始化，Huffman编码的生成等；</li>
  <li>多线程模型训练。即利用Hierarchical Softmax 或者 Negative Sampling 方法对网络中的参数进行求解；</li>
  <li>最终结果的处理。即是否保存和以何种形式保存。</li>
</ul>

<p>对于以上的过程，可以由下图表示：</p>

<p><img src="/images/word2vec/code_seq.png" alt="word2vec 代码实现流程" /></p>

<h3 id="1预处理">1、预处理</h3>
<p>在预处理部分，对word2vec需要使用的参数进行初始化，在word2vec中是利用传入的方式对参数进行初始化的。</p>

<p>在预处理部分，实现了sigmoid函数值的近似计算。在利用神经网络模型对样本进行预测的过程中，需要对其进行预测，此时，需要使用到sigmoid函数，sigmoid函数的具体形式为：</p>

<p>\begin{equation}
\sigma = \frac{1}{1+ e^{-x}} = \frac{e^{-x}}{1 + e^{-x}}
\end{equation}</p>

<p>如果每一次都请求计算<code class="highlighter-rouge">sigmoid</code>值，对性能将会有一定的影响，当<code class="highlighter-rouge">sigmoid</code>的值对精度的要求并不是非常严格时，可以采用近似计算。在<code class="highlighter-rouge">word2vec</code>中，将区间<code class="highlighter-rouge">[−6,6]</code>（设置的参数<code class="highlighter-rouge">MAX_EXP</code>为6）等距离划分成<code class="highlighter-rouge">EXP_TABLE_SIZE</code>等份，并将每个区间中的<code class="highlighter-rouge">sigmoid</code>值计算好存入到数组<code class="highlighter-rouge">expTable</code>中，需要使用时，直接从数组中查找。计算<code class="highlighter-rouge">sigmoid</code>值的代码如下所示：</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">expTable</span> <span class="o">=</span> <span class="p">(</span><span class="n">real</span> <span class="o">*</span><span class="p">)</span><span class="n">malloc</span><span class="p">((</span><span class="n">EXP_TABLE_SIZE</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="n">real</span><span class="p">));</span><span class="c1">// 申请EXP_TABLE_SIZE+1个空间</span>
<span class="c1">// 计算sigmoid值</span>
<span class="k">for</span> <span class="p">(</span><span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">EXP_TABLE_SIZE</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">expTable</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">exp</span><span class="p">((</span><span class="n">i</span> <span class="o">/</span> <span class="p">(</span><span class="n">real</span><span class="p">)</span><span class="n">EXP_TABLE_SIZE</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">MAX_EXP</span><span class="p">);</span> <span class="c1">// Precompute the exp() table</span>
        <span class="n">expTable</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">expTable</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">/</span> <span class="p">(</span><span class="n">expTable</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">);</span>                   <span class="c1">// Precompute f(x) = x / (x + 1)</span>
<span class="p">}</span>
</code></pre></div></div>

<blockquote>
  <p>注意：在上述代码中，作者使用的是小于EXP_TABLE_SIZE，实际的区间是<code class="highlighter-rouge">[−6,6)</code>。</p>
</blockquote>

<h3 id="2构建词库">2、构建词库</h3>
<p>在word2vec源码中，提供了两种构建词库的方法，分别为：</p>

<ul>
  <li>指定词库：ReadVocab()方法</li>
  <li>从词的文本构建词库：LearnVocabFromTrainFile()方法</li>
</ul>

<h4 id="21构建词库的过程">2.1、构建词库的过程</h4>
<p>在这里，我们以从词的文本构建词库为例。构建词库的过程如下所示：</p>

<p><img src="/images/word2vec/vocab.png" alt="构建词库" /></p>

<p>在这部分中，最主要的工作是对文本进行处理，包括低频词的处理，hash表的处理等等。首先，会在词库中增加一个“&lt; /s&gt;”的词，同时，在读取文本的过程中，将换行符“\n”也表示成该该词，如：</p>
<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="p">(</span><span class="n">ch</span> <span class="o">==</span> <span class="sc">'\n'</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">strcpy</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="p">(</span><span class="kt">char</span> <span class="o">*</span><span class="p">)</span><span class="s">"&lt;/s&gt;"</span><span class="p">);</span><span class="c1">// 换行符用&lt;/s&gt;表示</span>
    <span class="k">return</span><span class="p">;</span>
</code></pre></div></div>

<p>在循环的过程中，不断去读取文件中的每一个词，并在词库中进行查找，若存在该词，则该词的词频+1，否则，在词库中增加该词。在词库中，是通过哈希表的形式存储的。最终，会过滤掉一些低频词。</p>

<p>在得到最终的词库之前，还需根据词库中的词频(由高到低)对词库中的词进行排序。</p>

<h4 id="22对词的哈希处理">2.2、对词的哈希处理</h4>
<p>在存储词的过程中，同时保留这两个数组：</p>

<ul>
  <li>存储词的vocab</li>
  <li>存储词的hash的vocab_hash
其中，在vocab中，存储的是词对应的结构体：
    <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// 词的结构体</span>
<span class="k">struct</span> <span class="n">vocab_word</span> <span class="p">{</span>
      <span class="kt">long</span> <span class="kt">long</span> <span class="n">cn</span><span class="p">;</span> <span class="c1">// 出现的次数</span>
      <span class="kt">int</span> <span class="o">*</span><span class="n">point</span><span class="p">;</span> <span class="c1">// 从根结点到叶子节点的路径</span>
      <span class="kt">char</span> <span class="o">*</span><span class="n">word</span><span class="p">,</span> <span class="o">*</span><span class="n">code</span><span class="p">,</span> <span class="n">codelen</span><span class="p">;</span><span class="c1">// 分别对应着词，Huffman编码，编码长度</span>
<span class="p">};</span>
</code></pre></div>    </div>
    <p>在vocab_hash中存储的值是词在词库中的Index。</p>
  </li>
</ul>

<p>在对词的处理过程中，主要包括：</p>

<ul>
  <li>计算词的hash值：
    <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// 取词的hash值</span>
<span class="kt">int</span> <span class="nf">GetWordHash</span><span class="p">(</span><span class="kt">char</span> <span class="o">*</span><span class="n">word</span><span class="p">)</span> <span class="p">{</span>
      <span class="kt">unsigned</span> <span class="kt">long</span> <span class="kt">long</span> <span class="n">a</span><span class="p">,</span> <span class="n">hash</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
      <span class="k">for</span> <span class="p">(</span><span class="n">a</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">a</span> <span class="o">&lt;</span> <span class="n">strlen</span><span class="p">(</span><span class="n">word</span><span class="p">);</span> <span class="n">a</span><span class="o">++</span><span class="p">)</span> <span class="n">hash</span> <span class="o">=</span> <span class="n">hash</span> <span class="o">*</span> <span class="mi">257</span> <span class="o">+</span> <span class="n">word</span><span class="p">[</span><span class="n">a</span><span class="p">];</span>
      <span class="n">hash</span> <span class="o">=</span> <span class="n">hash</span> <span class="o">%</span> <span class="n">vocab_hash_size</span><span class="p">;</span>
      <span class="k">return</span> <span class="n">hash</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div>    </div>
  </li>
  <li>检索词是否存在。如不存在则返回-1，否则，返回该词在词库中的索引：
    <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">while</span> <span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">vocab_hash</span><span class="p">[</span><span class="n">hash</span><span class="p">]</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="k">return</span> <span class="o">-</span><span class="mi">1</span><span class="p">;</span><span class="c1">// 不存在该词</span>
  <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">strcmp</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">vocab</span><span class="p">[</span><span class="n">vocab_hash</span><span class="p">[</span><span class="n">hash</span><span class="p">]].</span><span class="n">word</span><span class="p">))</span> <span class="k">return</span> <span class="n">vocab_hash</span><span class="p">[</span><span class="n">hash</span><span class="p">];</span><span class="c1">// 返回索引值</span>
  <span class="n">hash</span> <span class="o">=</span> <span class="p">(</span><span class="n">hash</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">vocab_hash_size</span><span class="p">;</span><span class="c1">// 处理冲突</span>
<span class="p">}</span>
<span class="k">return</span> <span class="o">-</span><span class="mi">1</span><span class="p">;</span><span class="c1">// 不存在该词</span>
</code></pre></div>    </div>
    <p>在这个过程中，使用到了线性探测的开放定址法处理冲突，开放定址法就是一旦发生冲突，就去寻找下一个空的散列地址。</p>
  </li>
  <li>不存在，则插入新词。</li>
</ul>

<p>在这个过程中，除了需要将词增加到词库中，好需要计算该词的hash值，并将vocab_hash数组中的值标记为索引。</p>

<h4 id="23对低频词的处理">2.3、对低频词的处理</h4>
<p>在循环读取每一个词的过程中，当出现“vocab_size &gt; vocab_hash_size * 0.7”时，需要对低频词进行处理。其中，vocab_size表示的是目前词库中词的个数，vocab_hash_size表示的是初始设定的hash表的大小。</p>

<p>在处理低频词的过程中，通过参数“min_reduce”来控制，若词出现的次数小于等于该值时，则从词库中删除该词。</p>

<p>在删除了低频词后，需要重新对词库中的词进行hash值的计算。</p>

<h4 id="24根据词频对词库中的词排序">2.4、根据词频对词库中的词排序</h4>
<p>基于以上的过程，程序已经将词从文件中提取出来，并存入到指定的词库中（vocab数组），接下来，需要根据每一个词的词频对词库中的词按照词频从大到小排序，其基本过程在函数SortVocab中，排序过程为：</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">qsort</span><span class="p">(</span><span class="o">&amp;</span><span class="n">vocab</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">vocab_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="k">struct</span> <span class="n">vocab_word</span><span class="p">),</span> <span class="n">VocabCompare</span><span class="p">);</span>
</code></pre></div></div>

<p>保持字符“&lt; \s&gt;”在最开始的位置。排序后，根据“min_count”对低频词进行处理，与上述一样，再对剩下的词重新计算hash值。</p>

<p>至此，整个对词的处理过程就已经结束了。接下来，将是对网络结构的处理和词向量的训练。</p>

<h3 id="3初始化网络结构">3、初始化网络结构</h3>

<p>有了以上的对词的处理，就已经处理好了所有的训练样本，此时，便可以开始网络结构的初始化和接下来的网络训练。网络的初始化的过程在InitNet()函数中完成。</p>

<h4 id="31初始化网络参数">3.1、初始化网络参数</h4>
<p>在初始化的过程中，主要的参数包括词向量的初始化和映射层到输出层的权重的初始化，如下图所示：</p>

<p><img src="/images/word2vec/net.png" alt="网络参数" /></p>

<p>在初始化的过程中，映射层到输出层的权重都初始化为0.0，而对于每一个词向量的初始化，作者的初始化方法如下代码所示：</p>
<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="p">(</span><span class="n">a</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">a</span> <span class="o">&lt;</span> <span class="n">vocab_size</span><span class="p">;</span> <span class="n">a</span><span class="o">++</span><span class="p">)</span> <span class="k">for</span> <span class="p">(</span><span class="n">b</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">b</span> <span class="o">&lt;</span> <span class="n">layer1_size</span><span class="p">;</span> <span class="n">b</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">next_random</span> <span class="o">=</span> <span class="n">next_random</span> <span class="o">*</span> <span class="p">(</span><span class="kt">unsigned</span> <span class="kt">long</span> <span class="kt">long</span><span class="p">)</span><span class="mi">25214903917</span> <span class="o">+</span> <span class="mi">11</span><span class="p">;</span>
    <span class="c1">// 1、与：相当于将数控制在一定范围内</span>
    <span class="c1">// 2、0xFFFF：65536</span>
    <span class="c1">// 3、/65536：[0,1]之间</span>
    <span class="n">syn0</span><span class="p">[</span><span class="n">a</span> <span class="o">*</span> <span class="n">layer1_size</span> <span class="o">+</span> <span class="n">b</span><span class="p">]</span> <span class="o">=</span> <span class="p">(((</span><span class="n">next_random</span> <span class="o">&amp;</span> <span class="mh">0xFFFF</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">real</span><span class="p">)</span><span class="mi">65536</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">/</span> <span class="n">layer1_size</span><span class="p">;</span><span class="c1">// 初始化词向量</span>
<span class="p">}</span>
</code></pre></div></div>

<p>首先，生成一个很大的<code class="highlighter-rouge">next_random</code>的数，通过与“0xFFFF”进行与运算截断，再除以65536得到<code class="highlighter-rouge">[0,1]</code>之间的数，最终，得到的初始化的向量的范围为：<code class="highlighter-rouge">[−0.5m,0.5m]</code>，其中，mm为词向量的长度。</p>

<h4 id="32huffman树的构建">3.2、Huffman树的构建</h4>
<p>在层次Softmax中需要使用到Huffman树以及Huffman编码，因此，在网络结构的初始化过程中，也需要初始化Huffman树。在生成Huffman树的过程中，首先定义了33个长度为vocab_size*2+1的数组：</p>
<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">long</span> <span class="kt">long</span> <span class="o">*</span><span class="n">count</span> <span class="o">=</span> <span class="p">(</span><span class="kt">long</span> <span class="kt">long</span> <span class="o">*</span><span class="p">)</span><span class="n">calloc</span><span class="p">(</span><span class="n">vocab_size</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">long</span> <span class="kt">long</span><span class="p">));</span>
<span class="kt">long</span> <span class="kt">long</span> <span class="o">*</span><span class="n">binary</span> <span class="o">=</span> <span class="p">(</span><span class="kt">long</span> <span class="kt">long</span> <span class="o">*</span><span class="p">)</span><span class="n">calloc</span><span class="p">(</span><span class="n">vocab_size</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">long</span> <span class="kt">long</span><span class="p">));</span>
<span class="kt">long</span> <span class="kt">long</span> <span class="o">*</span><span class="n">parent_node</span> <span class="o">=</span> <span class="p">(</span><span class="kt">long</span> <span class="kt">long</span> <span class="o">*</span><span class="p">)</span><span class="n">calloc</span><span class="p">(</span><span class="n">vocab_size</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">long</span> <span class="kt">long</span><span class="p">));</span>
</code></pre></div></div>
<p>其中，count数组中前vocab_size存储的是每一个词的对应的词频，后面初始化的是很大的数，已知词库中的词是按照降序排列的，因此，构建Huffman树的过程如下所示：</p>

<p><img src="/images/word2vec/huff_1.png" alt="huffman 数据结构" /></p>

<p>首先，设置两个指针pos1和pos2，分别指向最后一个词和最后一个词的后一位，从两个指针所指的数中选择出最小的值，记为min1i，如pos1所指的值最小，此时，将pos1左移，再比较pos1和pos2所指的数，选择出最小的值，记为min2i，将他们的和存储到pos2所指的位置。并将此时pos2所指的位置设置为min1i和min2i的父节点，同时，记min2i所指的位置的编码为1，如下代码所示：</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// 设置父节点</span>
<span class="n">parent_node</span><span class="p">[</span><span class="n">min1i</span><span class="p">]</span> <span class="o">=</span> <span class="n">vocab_size</span> <span class="o">+</span> <span class="n">a</span><span class="p">;</span>
<span class="n">parent_node</span><span class="p">[</span><span class="n">min2i</span><span class="p">]</span> <span class="o">=</span> <span class="n">vocab_size</span> <span class="o">+</span> <span class="n">a</span><span class="p">;</span>
<span class="n">binary</span><span class="p">[</span><span class="n">min2i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span><span class="c1">// 设置一个子树的编码为1</span>
</code></pre></div></div>

<p>构建好Huffman树后，此时，需要根据构建好的Huffman树生成对应节点的Huffman编码。假设，上述的数据生成的最终的Huffman树为：</p>

<p><img src="/images/word2vec/huff_2.png" alt="huffman树" /></p>

<p>此时，count数组，binary数组和parent_node数组分别为：</p>

<p><img src="/images/word2vec/huff_3.png" alt="数组值" /></p>

<p>在生成Huffman编码的过程中，针对每一个词（词都在叶子节点上），从叶子节点开始，将编码存入到code数组中，如对于上图中的“R”节点来说，其code数组为{1,0}，再对其反转便是Huffman编码：</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">vocab</span><span class="p">[</span><span class="n">a</span><span class="p">].</span><span class="n">codelen</span> <span class="o">=</span> <span class="n">i</span><span class="p">;</span><span class="c1">// 词的编码长度</span>
<span class="n">vocab</span><span class="p">[</span><span class="n">a</span><span class="p">].</span><span class="n">point</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">vocab_size</span> <span class="o">-</span> <span class="mi">2</span><span class="p">;</span>
<span class="k">for</span> <span class="p">(</span><span class="n">b</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">b</span> <span class="o">&lt;</span> <span class="n">i</span><span class="p">;</span> <span class="n">b</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">vocab</span><span class="p">[</span><span class="n">a</span><span class="p">].</span><span class="n">code</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="n">b</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">code</span><span class="p">[</span><span class="n">b</span><span class="p">];</span><span class="c1">// 编码的反转</span>
    <span class="n">vocab</span><span class="p">[</span><span class="n">a</span><span class="p">].</span><span class="n">point</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="n">b</span><span class="p">]</span> <span class="o">=</span> <span class="n">point</span><span class="p">[</span><span class="n">b</span><span class="p">]</span> <span class="o">-</span> <span class="n">vocab_size</span><span class="p">;</span><span class="c1">// 记录的是从根结点到叶子节点的路径</span>
<span class="p">}</span>
</code></pre></div></div>

<blockquote>
  <p>注意：这里的Huffman树的构建和Huffman编码的生成过程写得比较精简。</p>
</blockquote>

<h4 id="33负样本选中表的初始化">3.3、负样本选中表的初始化</h4>
<p>如果是采用负采样的方法，此时还需要初始化每个词被选中的概率。在所有的词构成的词典中，每一个词出现的频率有高有低，我们希望，对于那些高频的词，被选中成为负样本的概率要大点，同时，对于那些出现频率比较低的词，我们希望其被选中成为负样本的频率低点。这个原理于“轮盘赌”的策略一致（详细可以参见“优化算法——遗传算法”）。在程序中，实现这部分功能的代码为：</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// 生成负采样的概率表</span>
<span class="kt">void</span> <span class="nf">InitUnigramTable</span><span class="p">()</span> <span class="p">{</span>
        <span class="kt">int</span> <span class="n">a</span><span class="p">,</span> <span class="n">i</span><span class="p">;</span>
        <span class="kt">double</span> <span class="n">train_words_pow</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
        <span class="kt">double</span> <span class="n">d1</span><span class="p">,</span> <span class="n">power</span> <span class="o">=</span> <span class="mf">0.75</span><span class="p">;</span>
        <span class="n">table</span> <span class="o">=</span> <span class="p">(</span><span class="kt">int</span> <span class="o">*</span><span class="p">)</span><span class="n">malloc</span><span class="p">(</span><span class="n">table_size</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">));</span><span class="c1">// int --&gt; int</span>
        <span class="k">for</span> <span class="p">(</span><span class="n">a</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">a</span> <span class="o">&lt;</span> <span class="n">vocab_size</span><span class="p">;</span> <span class="n">a</span><span class="o">++</span><span class="p">)</span> <span class="n">train_words_pow</span> <span class="o">+=</span> <span class="n">pow</span><span class="p">(</span><span class="n">vocab</span><span class="p">[</span><span class="n">a</span><span class="p">].</span><span class="n">cn</span><span class="p">,</span> <span class="n">power</span><span class="p">);</span>
        <span class="c1">// 类似轮盘赌生成每个词的概率</span>
        <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
        <span class="n">d1</span> <span class="o">=</span> <span class="n">pow</span><span class="p">(</span><span class="n">vocab</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">cn</span><span class="p">,</span> <span class="n">power</span><span class="p">)</span> <span class="o">/</span> <span class="n">train_words_pow</span><span class="p">;</span>
        <span class="k">for</span> <span class="p">(</span><span class="n">a</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">a</span> <span class="o">&lt;</span> <span class="n">table_size</span><span class="p">;</span> <span class="n">a</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
                <span class="n">table</span><span class="p">[</span><span class="n">a</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span><span class="p">;</span>
                <span class="k">if</span> <span class="p">(</span><span class="n">a</span> <span class="o">/</span> <span class="p">(</span><span class="kt">double</span><span class="p">)</span><span class="n">table_size</span> <span class="o">&gt;</span> <span class="n">d1</span><span class="p">)</span> <span class="p">{</span>
                        <span class="n">i</span><span class="o">++</span><span class="p">;</span>
                        <span class="n">d1</span> <span class="o">+=</span> <span class="n">pow</span><span class="p">(</span><span class="n">vocab</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">cn</span><span class="p">,</span> <span class="n">power</span><span class="p">)</span> <span class="o">/</span> <span class="n">train_words_pow</span><span class="p">;</span>
                <span class="p">}</span>
                <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&gt;=</span> <span class="n">vocab_size</span><span class="p">)</span> <span class="n">i</span> <span class="o">=</span> <span class="n">vocab_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">;</span>
        <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<p>在实现的过程中，没有直接使用每一个词的频率，而是使用了词的0.75次方。</p>

<h3 id="4多线程模型训练">4、多线程模型训练</h3>
<p>以上的各个部分是为训练词向量做准备，即准备训练数据，构建训练模型。在上述的初始化完成后，接下来就是根据不同的方法对模型进行训练，在实现的过程中，作者使用了多线程的方法对其进行训练。</p>

<h4 id="41多线程的处理">4.1、多线程的处理</h4>
<p>为了能够对文本进行加速训练，在实现的过程中，作者使用了多线程的方法，并对每一个线程上分配指定大小的文件：</p>
<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// 利用多线程对训练文件划分，每个线程训练一部分的数据</span>
<span class="n">fseek</span><span class="p">(</span><span class="n">fi</span><span class="p">,</span> <span class="n">file_size</span> <span class="o">/</span> <span class="p">(</span><span class="kt">long</span> <span class="kt">long</span><span class="p">)</span><span class="n">num_threads</span> <span class="o">*</span> <span class="p">(</span><span class="kt">long</span> <span class="kt">long</span><span class="p">)</span><span class="n">id</span><span class="p">,</span> <span class="n">SEEK_SET</span><span class="p">);</span>
</code></pre></div></div>
<p>注意：这边的多线程分割方式并不能保证每一个线程分到的文件是互斥的。对于其中的原因，可以参见“Linux C 编程——多线程”。</p>

<p>这个过程可以通过下图简单的描述：</p>

<p><img src="/images/word2vec/pthread.png" alt="多线程处理" /></p>

<p>在实现多线程的过程中，作者并没有加锁的操作，而是对模型参数和词向量的修改可以任意执行，这一点类似于基于随机梯度的方法，训练的过程与训练样本的训练是没有关系的，这样可以大大加快对词向量的训练。抛开多线程的部分，在每一个线程内执行的是对模型和词向量的训练。</p>

<p>作者在实现的过程中，主要实现了两个模型，即CBOW模型和Skip-gram模型，在每个模型中，又分别使用到了两种不同的训练方法，即层次Softmax和Negative Sampling方法。</p>

<p>对于CBOW模型和Skip-gram模型的理解，首先必须知道统计语言模型（Statistic Language Model）。</p>

<p>在统计语言模型中的核心内容是：$\underline{计算一组词语能够成为一个句子的概率}$。</p>

<p>为了能够求解其中的参数，一大批参数求解的方法被提出，在其中，就有word2vec中要使用的神经概率语言模型。具体的神经概率语言模型可以参见“”。</p>

<h4 id="42cbow模型">4.2、CBOW模型</h4>

<p>CBOW模型和Skip-gram模型是神经概率语言模型的两种变形形式，其中，在CBOW模型中包含三层，即输入层，映射层和输出层。对于CBOW模型，如下图所示：</p>

<p><img src="/images/word2vec/cbow.png" alt="cbow模型" /></p>

<p>在CBOW模型中，通过词$w_t$的前后词$w_{t-2},w_{t-1},w_{t+1},w_{t+2}$来预测当前词$w_t$。此处的窗口的大小 $window = 2$。</p>

<h4 id="43-参数更新过程">4.3 参数更新过程</h4>
<ul>
  <li>前向的处理 + 反向损失梯度传播</li>
</ul>

<h5 id="431从输入层到映射层">4.3.1、从输入层到映射层</h5>
<p>首先找到每个词对应的词向量，并将这些词的词向量相加，程序代码如下所示：</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>// in -&gt; hidden
// 输入层到映射层
cw = 0;
for (a = b; a &lt; window * 2 + 1 - b; a++) if (a != window) {
    c = sentence_position - window + a;// sentence_position表示的是当前的位置
    // 判断c是否越界
    if (c &lt; 0) continue;
    if (c &gt;= sentence_length) continue;

    last_word = sen[c];// 找到c对应的索引
    if (last_word == -1) continue;

    for (c = 0; c &lt; layer1_size; c++) neu1[c] += syn0[c + last_word * layer1_size];// 累加
    cw++;
}
</code></pre></div></div>

<p>当累加完窗口内的所有的词向量的之后，存储在映射层neu1中，并取平均，程序代码如下所示：</p>
<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="p">(</span><span class="n">c</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">c</span> <span class="o">&lt;</span> <span class="n">layer1_size</span><span class="p">;</span> <span class="n">c</span><span class="o">++</span><span class="p">)</span> <span class="n">neu1</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="o">/=</span> <span class="n">cw</span><span class="p">;</span><span class="c1">// 计算均值</span>
</code></pre></div></div>
<p>当取得了映射层的结果后，此时就需要使用Hierarchical Softmax或者Negative Sampling对模型进行训练。</p>

<h5 id="432hierarchical-softmax">4.3.2、Hierarchical Softmax</h5>

<p><img src="/images/word2vec/hs.png" alt="hs模型的数学原理" /></p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="p">(</span><span class="n">d</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">d</span> <span class="o">&lt;</span> <span class="n">vocab</span><span class="p">[</span><span class="n">word</span><span class="p">].</span><span class="n">codelen</span><span class="p">;</span> <span class="n">d</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span><span class="c1">// word为当前词</span>
    <span class="c1">// 计算输出层的输出</span>
    <span class="n">f</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="n">l2</span> <span class="o">=</span> <span class="n">vocab</span><span class="p">[</span><span class="n">word</span><span class="p">].</span><span class="n">point</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">*</span> <span class="n">layer1_size</span><span class="p">;</span><span class="c1">// 找到第d个词对应的权重</span>
    <span class="c1">// Propagate hidden -&gt; output</span>
    <span class="k">for</span> <span class="p">(</span><span class="n">c</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">c</span> <span class="o">&lt;</span> <span class="n">layer1_size</span><span class="p">;</span> <span class="n">c</span><span class="o">++</span><span class="p">)</span> <span class="n">f</span> <span class="o">+=</span> <span class="n">neu1</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="o">*</span> <span class="n">syn1</span><span class="p">[</span><span class="n">c</span> <span class="o">+</span> <span class="n">l2</span><span class="p">];</span><span class="c1">// 映射层到输出层</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">f</span> <span class="o">&lt;=</span> <span class="o">-</span><span class="n">MAX_EXP</span><span class="p">)</span> <span class="k">continue</span><span class="p">;</span>
    <span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="n">f</span> <span class="o">&gt;=</span> <span class="n">MAX_EXP</span><span class="p">)</span> <span class="k">continue</span><span class="p">;</span>
    <span class="k">else</span> <span class="n">f</span> <span class="o">=</span> <span class="n">expTable</span><span class="p">[(</span><span class="kt">int</span><span class="p">)((</span><span class="n">f</span> <span class="o">+</span> <span class="n">MAX_EXP</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">EXP_TABLE_SIZE</span> <span class="o">/</span> <span class="n">MAX_EXP</span> <span class="o">/</span> <span class="mi">2</span><span class="p">))];</span><span class="c1">// Sigmoid结果</span>

    <span class="c1">// 'g' is the gradient multiplied by the learning rate</span>
    <span class="n">g</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">vocab</span><span class="p">[</span><span class="n">word</span><span class="p">].</span><span class="n">code</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">-</span> <span class="n">f</span><span class="p">)</span> <span class="o">*</span> <span class="n">alpha</span><span class="p">;</span>
    <span class="c1">// Propagate errors output -&gt; hidden</span>
    <span class="k">for</span> <span class="p">(</span><span class="n">c</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">c</span> <span class="o">&lt;</span> <span class="n">layer1_size</span><span class="p">;</span> <span class="n">c</span><span class="o">++</span><span class="p">)</span> <span class="n">neu1e</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="o">+=</span> <span class="n">g</span> <span class="o">*</span> <span class="n">syn1</span><span class="p">[</span><span class="n">c</span> <span class="o">+</span> <span class="n">l2</span><span class="p">];</span><span class="c1">// 修改映射后的结果</span>
    <span class="c1">// Learn weights hidden -&gt; output</span>
    <span class="k">for</span> <span class="p">(</span><span class="n">c</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">c</span> <span class="o">&lt;</span> <span class="n">layer1_size</span><span class="p">;</span> <span class="n">c</span><span class="o">++</span><span class="p">)</span> <span class="n">syn1</span><span class="p">[</span><span class="n">c</span> <span class="o">+</span> <span class="n">l2</span><span class="p">]</span> <span class="o">+=</span> <span class="n">g</span> <span class="o">*</span> <span class="n">neu1</span><span class="p">[</span><span class="n">c</span><span class="p">];</span><span class="c1">// 修改映射层到输出层之间的权重</span>
<span class="p">}</span>
</code></pre></div></div>

<p>对于窗口内的词的向量的更新，则是利用窗口内的所有词的梯度之和$neu1e = \sum{\frac{\partial{L(w,j)}}{\partial{X_w}}}$ 来更新，如程序代码所示：</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// hidden -&gt; in</span>
<span class="c1">// 以上是从映射层到输出层的修改，现在返回修改每一个词向量</span>
<span class="k">for</span> <span class="p">(</span><span class="n">a</span> <span class="o">=</span> <span class="n">b</span><span class="p">;</span> <span class="n">a</span> <span class="o">&lt;</span> <span class="n">window</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">b</span><span class="p">;</span> <span class="n">a</span><span class="o">++</span><span class="p">)</span> <span class="k">if</span> <span class="p">(</span><span class="n">a</span> <span class="o">!=</span> <span class="n">window</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">sentence_position</span> <span class="o">-</span> <span class="n">window</span> <span class="o">+</span> <span class="n">a</span><span class="p">;</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">c</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">)</span> <span class="k">continue</span><span class="p">;</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">c</span> <span class="o">&gt;=</span> <span class="n">sentence_length</span><span class="p">)</span> <span class="k">continue</span><span class="p">;</span>
    <span class="n">last_word</span> <span class="o">=</span> <span class="n">sen</span><span class="p">[</span><span class="n">c</span><span class="p">];</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">last_word</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="k">continue</span><span class="p">;</span>
    <span class="c1">// 利用窗口内的所有词向量的梯度之和来更新</span>
    <span class="k">for</span> <span class="p">(</span><span class="n">c</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">c</span> <span class="o">&lt;</span> <span class="n">layer1_size</span><span class="p">;</span> <span class="n">c</span><span class="o">++</span><span class="p">)</span> <span class="n">syn0</span><span class="p">[</span><span class="n">c</span> <span class="o">+</span> <span class="n">last_word</span> <span class="o">*</span> <span class="n">layer1_size</span><span class="p">]</span> <span class="o">+=</span> <span class="n">neu1e</span><span class="p">[</span><span class="n">c</span><span class="p">];</span>
<span class="p">}</span>
</code></pre></div></div>

<h5 id="433negative-sampling">4.3.3、Negative Sampling</h5>
<p>与Hierarchical Softmax一致，Negative Sampling也是一种加速计算的方法，在Negative Sampling方法中使用的是随机的负采样，在CBOW模型中，已知词$w$的上下文$context(w)$，需要预测词$w$，对于给定的上下文$context(w)$，词$w$即为正样本，其他的样本为负样本，此时我们需要根据词频从剩下的词中挑选出最合适的负样本，实现的代码如下所示：</p>
<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// 标记target和label</span>
<span class="k">if</span> <span class="p">(</span><span class="n">d</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span><span class="c1">// 正样本</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">word</span><span class="p">;</span>
    <span class="n">label</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
<span class="p">}</span> <span class="k">else</span> <span class="p">{</span><span class="c1">// 选择出负样本</span>
    <span class="n">next_random</span> <span class="o">=</span> <span class="n">next_random</span> <span class="o">*</span> <span class="p">(</span><span class="kt">unsigned</span> <span class="kt">long</span> <span class="kt">long</span><span class="p">)</span><span class="mi">25214903917</span> <span class="o">+</span> <span class="mi">11</span><span class="p">;</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">table</span><span class="p">[(</span><span class="n">next_random</span> <span class="o">&gt;&gt;</span> <span class="mi">16</span><span class="p">)</span> <span class="o">%</span> <span class="n">table_size</span><span class="p">];</span><span class="c1">// 从table表中选择出负样本</span>
    <span class="c1">// 重新选择</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">target</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="n">target</span> <span class="o">=</span> <span class="n">next_random</span> <span class="o">%</span> <span class="p">(</span><span class="n">vocab_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">;</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">target</span> <span class="o">==</span> <span class="n">word</span><span class="p">)</span> <span class="k">continue</span><span class="p">;</span>
    <span class="n">label</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>
<p><img src="/images/word2vec/neg.png" alt="neg模型的数学原理" /></p>

<p>代码更新为：</p>
<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="p">(</span><span class="n">f</span> <span class="o">&gt;</span> <span class="n">MAX_EXP</span><span class="p">)</span> <span class="n">g</span> <span class="o">=</span> <span class="p">(</span><span class="n">label</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">alpha</span><span class="p">;</span>
<span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="n">f</span> <span class="o">&lt;</span> <span class="o">-</span><span class="n">MAX_EXP</span><span class="p">)</span> <span class="n">g</span> <span class="o">=</span> <span class="p">(</span><span class="n">label</span> <span class="o">-</span> <span class="mi">0</span><span class="p">)</span> <span class="o">*</span> <span class="n">alpha</span><span class="p">;</span>
<span class="k">else</span> <span class="n">g</span> <span class="o">=</span> <span class="p">(</span><span class="n">label</span> <span class="o">-</span> <span class="n">expTable</span><span class="p">[(</span><span class="kt">int</span><span class="p">)((</span><span class="n">f</span> <span class="o">+</span> <span class="n">MAX_EXP</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">EXP_TABLE_SIZE</span> <span class="o">/</span> <span class="n">MAX_EXP</span> <span class="o">/</span> <span class="mi">2</span><span class="p">))])</span> <span class="o">*</span> <span class="n">alpha</span><span class="p">;</span>

<span class="k">for</span> <span class="p">(</span><span class="n">c</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">c</span> <span class="o">&lt;</span> <span class="n">layer1_size</span><span class="p">;</span> <span class="n">c</span><span class="o">++</span><span class="p">)</span> <span class="n">neu1e</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="o">+=</span> <span class="n">g</span> <span class="o">*</span> <span class="n">syn1neg</span><span class="p">[</span><span class="n">c</span> <span class="o">+</span> <span class="n">l2</span><span class="p">];</span>
<span class="k">for</span> <span class="p">(</span><span class="n">c</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">c</span> <span class="o">&lt;</span> <span class="n">layer1_size</span><span class="p">;</span> <span class="n">c</span><span class="o">++</span><span class="p">)</span> <span class="n">syn1neg</span><span class="p">[</span><span class="n">c</span> <span class="o">+</span> <span class="n">l2</span><span class="p">]</span> <span class="o">+=</span> <span class="n">g</span> <span class="o">*</span> <span class="n">neu1</span><span class="p">[</span><span class="n">c</span><span class="p">];</span>
</code></pre></div></div>
<p>对词向量的更新与Hierarchical Softmax中一致。</p>

<h4 id="43skip-gram模型">4.3、Skip-gram模型</h4>

<p>而Skip-gram模型与CBOW正好相反，在Skip-gram模型中，则是通过当前词$w_t$来预测其前后词$w_{t-2},w_{t-1},w_{t+1},w_{t+2}$，Skip-gram模型如下图所示：</p>

<p><img src="/images/word2vec/skg.png" alt="skip-gram 模型" /></p>

<h5 id="431hierarchical-softmax">4.3.1、Hierarchical Softmax</h5>

<p>由上述的分析，我们发现，在Skip-gram模型中，其计算方法与CBOW模型很相似，不同的是，在Skip-gram模型中，需要使用当前词分别预测窗口中的词，因此，这是一个循环的过程：</p>
<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="p">(</span><span class="n">a</span> <span class="o">=</span> <span class="n">b</span><span class="p">;</span> <span class="n">a</span> <span class="o">&lt;</span> <span class="n">window</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">b</span><span class="p">;</span> <span class="n">a</span><span class="o">++</span><span class="p">)</span> <span class="k">if</span> <span class="p">(</span><span class="n">a</span> <span class="o">!=</span> <span class="n">window</span><span class="p">)</span>
</code></pre></div></div>
<p>对于向量的更新过程与CBOW模型中的Hierarchical Softmax一致：</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">c</span> <span class="o">=</span> <span class="n">sentence_position</span> <span class="o">-</span> <span class="n">window</span> <span class="o">+</span> <span class="n">a</span><span class="p">;</span>
<span class="k">if</span> <span class="p">(</span><span class="n">c</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">)</span> <span class="k">continue</span><span class="p">;</span>
<span class="k">if</span> <span class="p">(</span><span class="n">c</span> <span class="o">&gt;=</span> <span class="n">sentence_length</span><span class="p">)</span> <span class="k">continue</span><span class="p">;</span>
<span class="n">last_word</span> <span class="o">=</span> <span class="n">sen</span><span class="p">[</span><span class="n">c</span><span class="p">];</span>
<span class="k">if</span> <span class="p">(</span><span class="n">last_word</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="k">continue</span><span class="p">;</span>
<span class="n">l1</span> <span class="o">=</span> <span class="n">last_word</span> <span class="o">*</span> <span class="n">layer1_size</span><span class="p">;</span>
<span class="k">for</span> <span class="p">(</span><span class="n">c</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">c</span> <span class="o">&lt;</span> <span class="n">layer1_size</span><span class="p">;</span> <span class="n">c</span><span class="o">++</span><span class="p">)</span> <span class="n">neu1e</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
<span class="c1">// HIERARCHICAL SOFTMAX</span>
<span class="k">if</span> <span class="p">(</span><span class="n">hs</span><span class="p">)</span> <span class="k">for</span> <span class="p">(</span><span class="n">d</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">d</span> <span class="o">&lt;</span> <span class="n">vocab</span><span class="p">[</span><span class="n">word</span><span class="p">].</span><span class="n">codelen</span><span class="p">;</span> <span class="n">d</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">f</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="n">l2</span> <span class="o">=</span> <span class="n">vocab</span><span class="p">[</span><span class="n">word</span><span class="p">].</span><span class="n">point</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">*</span> <span class="n">layer1_size</span><span class="p">;</span>
    <span class="c1">// Propagate hidden -&gt; output</span>
    <span class="c1">// 映射层即为输入层</span>
    <span class="k">for</span> <span class="p">(</span><span class="n">c</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">c</span> <span class="o">&lt;</span> <span class="n">layer1_size</span><span class="p">;</span> <span class="n">c</span><span class="o">++</span><span class="p">)</span> <span class="n">f</span> <span class="o">+=</span> <span class="n">syn0</span><span class="p">[</span><span class="n">c</span> <span class="o">+</span> <span class="n">l1</span><span class="p">]</span> <span class="o">*</span> <span class="n">syn1</span><span class="p">[</span><span class="n">c</span> <span class="o">+</span> <span class="n">l2</span><span class="p">];</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">f</span> <span class="o">&lt;=</span> <span class="o">-</span><span class="n">MAX_EXP</span><span class="p">)</span> <span class="k">continue</span><span class="p">;</span>
    <span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="n">f</span> <span class="o">&gt;=</span> <span class="n">MAX_EXP</span><span class="p">)</span> <span class="k">continue</span><span class="p">;</span>
    <span class="k">else</span> <span class="n">f</span> <span class="o">=</span> <span class="n">expTable</span><span class="p">[(</span><span class="kt">int</span><span class="p">)((</span><span class="n">f</span> <span class="o">+</span> <span class="n">MAX_EXP</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">EXP_TABLE_SIZE</span> <span class="o">/</span> <span class="n">MAX_EXP</span> <span class="o">/</span> <span class="mi">2</span><span class="p">))];</span>

    <span class="c1">// 'g' is the gradient multiplied by the learning rate</span>
    <span class="n">g</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">vocab</span><span class="p">[</span><span class="n">word</span><span class="p">].</span><span class="n">code</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">-</span> <span class="n">f</span><span class="p">)</span> <span class="o">*</span> <span class="n">alpha</span><span class="p">;</span>
    <span class="c1">// Propagate errors output -&gt; hidden</span>
    <span class="k">for</span> <span class="p">(</span><span class="n">c</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">c</span> <span class="o">&lt;</span> <span class="n">layer1_size</span><span class="p">;</span> <span class="n">c</span><span class="o">++</span><span class="p">)</span> <span class="n">neu1e</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="o">+=</span> <span class="n">g</span> <span class="o">*</span> <span class="n">syn1</span><span class="p">[</span><span class="n">c</span> <span class="o">+</span> <span class="n">l2</span><span class="p">];</span>
    <span class="c1">// Learn weights hidden -&gt; output</span>
    <span class="k">for</span> <span class="p">(</span><span class="n">c</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">c</span> <span class="o">&lt;</span> <span class="n">layer1_size</span><span class="p">;</span> <span class="n">c</span><span class="o">++</span><span class="p">)</span> <span class="n">syn1</span><span class="p">[</span><span class="n">c</span> <span class="o">+</span> <span class="n">l2</span><span class="p">]</span> <span class="o">+=</span> <span class="n">g</span> <span class="o">*</span> <span class="n">syn0</span><span class="p">[</span><span class="n">c</span> <span class="o">+</span> <span class="n">l1</span><span class="p">];</span>
<span class="p">}</span>
</code></pre></div></div>
<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// Learn weights input -&gt; hidden</span>
<span class="k">for</span> <span class="p">(</span><span class="n">c</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">c</span> <span class="o">&lt;</span> <span class="n">layer1_size</span><span class="p">;</span> <span class="n">c</span><span class="o">++</span><span class="p">)</span> <span class="n">syn0</span><span class="p">[</span><span class="n">c</span> <span class="o">+</span> <span class="n">l1</span><span class="p">]</span> <span class="o">+=</span> <span class="n">neu1e</span><span class="p">[</span><span class="n">c</span><span class="p">];</span>
</code></pre></div></div>

<h5 id="432negative-sampling">4.3.2、Negative Sampling</h5>
<p>与上述一致，在Skip-gram中与CBOW中的唯一不同是在Skip-gram中是循环的过程。代码的实现类似与上面的Hierarchical Softmax。</p>

<p>注释版的word2vec源码已经上传到Github中：Github：<a href="https://github.com/zhaozhiyong19890102/OpenSourceReading/blob/master/word2vec/word2vec.c">word2vec.c</a></p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>作者：zhiyong_will 
来源：CSDN 
原文：https://blog.csdn.net/google19890102/article/details/51887344 
版权声明：本文为博主原创文章，转载请附上博文链接！
</code></pre></div></div>

    <div class="ui horizontal divider">
        Thank You For Reading
    </div>
    <!-- author box -->
<div class="ui segment">
    <div class="ui items">
        <div class="item">
            <a class="ui circular tiny image">
                <img src="/assets/img/profile.jpg">
            </a>
            <div class="content">
                <a class="header">晋戈</a>
                <div class="description">
                    <p>同理心温暖世界，认真设计未来</p>
                </div>
            </div>
        </div>
    </div>
</div>

    <section class="disqus">
    <div id="disqus_thread"></div>
    <script type="text/javascript">

        var disqus_shortname = 'nagekar'; 
        var disqus_developer = 0; // developer mode is on
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = 'https://' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>
</div>
        </main>
    </div>
    
    <footer>
    <div class="footer-wrapper">
        <div class="ui secondary menu inverted">
            <p class="item">Wall-E &copy; 2017</p>
            <div class="right menu">
                <a class="item" href="/archive.html">
                    Latest Posts
                </a>
                <a class="item" href="https://twitter.com/fsf">
                    Twitter
                </a>
                <a class="item" href="https://github.com/nemo-tj">
                    Github
                </a>
            </div>
        </div>
    </div>
</footer>
<script type="text/javascript" src="/assets/js/main.js"></script>
<script type="text/javascript" src="/assets/js/vendor/jquery.min.js"></script>
<script type="text/javascript" src="/assets/js/vendor/semantic.min.js"></script>

    <script src="/assets/js/vendor/jquery.min.js"></script>
    <script src="/assets/js/vendor/semantic.min.js"></script>
    <script type="/assets/js/main.js"></script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
    </script>
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML">
    </script>
    
</body>

</html>