<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Wall-E - From Space From Earth</title>
    <description>用心设计未来</description>
    <link>http://localhost:4000//</link>
    <atom:link href="http://localhost:4000//feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sat, 04 Jul 2020 15:04:20 +0800</pubDate>
    <lastBuildDate>Sat, 04 Jul 2020 15:04:20 +0800</lastBuildDate>
    <generator>Jekyll v3.7.3</generator>
    
      <item>
        <title>IOT中的深度学习和应用</title>
        <description>&lt;p&gt;（1）方案设计：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;定义：
    &lt;ul&gt;
      &lt;li&gt;节点：可以进行无线通信和收集传感器信息&lt;/li&gt;
      &lt;li&gt;协调器：节点和服务器通信的中介，将节点信息传给服务器，接收服务器指令&lt;/li&gt;
      &lt;li&gt;传感器：安装在节点上，用以采集信息，如红外检测、水流速度等
其中，节点搭载低功耗处理器和传感器，应用人工智能深度学习算法，具有信息采集、定位、处理到智能识别的一体化功能。节点可以大规模自由部署，动态组网，构建救援现场的”生命浮标网“。该方案从两方面来保证救援工作的开展：&lt;/li&gt;
      &lt;li&gt;收集受灾现场信息、检测受困人员的体征、位置信息&lt;/li&gt;
      &lt;li&gt;为指挥救援工作提供一线信息支持&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;（2）创新性：
应用智能识别算法，融合异构多元信息，进行实时识别和监测，保证救援的及时性和科学性。通过动态部署无线节点进行信息收集和预处理，相比于现有的信息收集手段，更加智能和鲁棒，极大的减少了搜救工作的难度。在低功耗节点上部署人工智能算法，在第一时间识别受灾人员的安全状况，为救援工作提供科学的数据支撑。&lt;/p&gt;

&lt;p&gt;（3）可行性：
当前人工智能算法应用在语音、图像识别等领域，已经取得显著效果，相关的研究和工业产品也在逐步成熟。但是这类应用由于本身对数据的依赖很强，算法模型的训练需要消耗大量的计算资源，对部署的平台资源要求较高。为解决模型效果和资源之间的矛盾，目前有学者进行了相关的研究，并取得了一定的进展。比如：通过对模型的压缩裁剪，降低了功耗和计算量，从而具备了落地应用的可能性。
无线传感网技术伴随物联网iot的发展，已经有了长足的进步。如面包屑传感网在消防救援中，已经初步落地。具备初期的技术积累和工程经验。
由此，在此类水灾救援中，搭建一个基于无线传感网的智能救援系统，是可行的，且具有现实意义的。&lt;/p&gt;

&lt;h2 id=&quot;面包屑系统应用于极端恶劣条件救援&quot;&gt;面包屑系统应用于极端恶劣条件救援&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;基础：低成本分布式的数据采集、处理、训练和学习
    &lt;ul&gt;
      &lt;li&gt;模型压缩和裁剪：在低功耗节点上进行深度学习框架的部署&lt;/li&gt;
      &lt;li&gt;样本数据生成和结果评估：半监督式生成数据标签、提高模型能力&lt;/li&gt;
      &lt;li&gt;多数据源融合算法：异构的数据源，统一特征抽取，模型训练和学习&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;可行性：
    &lt;ul&gt;
      &lt;li&gt;低功耗硬件+智能算法&lt;/li&gt;
      &lt;li&gt;端到端闭环IoT系统：分布式部署、智能组网、数据采集训练应用平台&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;先进性：
    &lt;ul&gt;
      &lt;li&gt;智能算法&lt;/li&gt;
      &lt;li&gt;分布式&lt;/li&gt;
      &lt;li&gt;端到端&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;deepsense-a-unified-deep-learning-framework-for-time-series-mobile-sensing-data-processing&quot;&gt;DeepSense: a Unified Deep Learning Framework for Time-Series Mobile Sensing Data Processing&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;多传感器数据融合，序列信息学习：空间（多个传感器输入） + 时间（时间序列）&lt;/li&gt;
  &lt;li&gt;CNN 提取空间特征，进行融合；&lt;/li&gt;
  &lt;li&gt;RNN 学习时间序列，进行预测&lt;/li&gt;
  &lt;li&gt;有监督学习：
    &lt;ul&gt;
      &lt;li&gt;回归问题：导航定位&lt;/li&gt;
      &lt;li&gt;分类问题：姿态识别&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;deepiot-compressing-deep-neural-network-structures-for-sensing-systems-with-a-compressor-critic-framework&quot;&gt;DeepIoT: Compressing Deep Neural Network Structures for Sensing Systems with a Compressor-Critic Framework&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;模型压缩，落地应用到低功耗端上&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;sensegan-enabling-deep-learning-for-internet-of-things-with-a-semi-supervised-framework&quot;&gt;SenseGAN: Enabling Deep Learning for Internet of Things with a Semi-Supervised Framework&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;半监督学习， 打标签&lt;/li&gt;
  &lt;li&gt;GAN: 生成对抗网络&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;fastdeepiot-towards-understanding-and-optimizing-neural-network-execution-time-on-mobile-and-embedded-devices&quot;&gt;FastDeepIoT: Towards Understanding and Optimizing Neural Network Execution Time on Mobile and Embedded Devices&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;In this paper, we show how a better understanding of the non-linear relation between neural network structure and performance can further improve execution time and energy consumption without impacting accuracy&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;stfnets-learning-sensing-signals-from-the-time-frequency-perspective-with-short-time-fourier-neural-networks&quot;&gt;STFNets: Learning Sensing Signals from the Time-Frequency Perspective with Short-Time Fourier Neural Networks&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;特征工程：&lt;/li&gt;
  &lt;li&gt;引入频域信号：傅立叶变化&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;apdeepsense-deep-learning-uncertainty-estimation-without-the-pain-for-iot-applications&quot;&gt;ApDeepSense: Deep Learning Uncertainty Estimation Without the Pain for IoT Applications&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Uncertainty Estimation&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;sadeepsense-self-attention-deep-learning-framework-for-heterogeneous-on-device-sensors-in-internet-of-things-applications&quot;&gt;SADeepSense: Self-Attention Deep Learning Framework for Heterogeneous On-Device Sensors in Internet of Things Applications&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Attention: 区分对待模型的输入源&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;eugene-towards-deep-intelligence-as-a-service&quot;&gt;Eugene: Towards Deep Intelligence as a Service&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;IOT 系统综述：深度学习的应用和挑战&lt;/li&gt;
  &lt;li&gt;Training and Data Labeling&lt;/li&gt;
  &lt;li&gt;Model Reduction and Caching&lt;/li&gt;
  &lt;li&gt;Execution Profiling&lt;/li&gt;
  &lt;li&gt;Result Quality Estimation&lt;/li&gt;
  &lt;li&gt;Run-time Inference&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;苏州中心智库&lt;/p&gt;

&lt;p&gt;机器人、消防头盔、中建公司&lt;/p&gt;

</description>
        <pubDate>Sun, 29 Mar 2020 00:00:00 +0800</pubDate>
        <link>http://localhost:4000//2020/03/IOT-Project-DeepLearning-Application.html</link>
        <guid isPermaLink="true">http://localhost:4000//2020/03/IOT-Project-DeepLearning-Application.html</guid>
        
        <category>IOT</category>
        
        
      </item>
    
      <item>
        <title>信息流推荐中的若干问题和思考</title>
        <description>&lt;h2 id=&quot;信息流产品的优化点&quot;&gt;信息流产品的优化点&lt;/h2&gt;

&lt;h3 id=&quot;供给侧--努力--数量质量效率&quot;&gt;供给侧 ( 努力 ): 数量、质量、效率&lt;/h3&gt;

&lt;p&gt;内容量大幅提升：主要来自于开放跟拍权限，加上低门槛创作工具和爆款内容的持续引流。&lt;/p&gt;

&lt;p&gt;内容审核效率的提升：机器审核准确率提高。&lt;/p&gt;

&lt;h3 id=&quot;运营侧--努力--丰富多元&quot;&gt;运营侧 ( 努力 ): 丰富多元&lt;/h3&gt;

&lt;p&gt;海外团队的本土化运营能力持续在增强，local团队对内容生态的把控，更丰富多元化&lt;/p&gt;

&lt;h3 id=&quot;营销侧--努力--品牌心智&quot;&gt;营销侧 ( 努力 ): 品牌心智&lt;/h3&gt;

&lt;p&gt;PR品牌的持续曝光以及泛娱乐战略资源的深入&lt;/p&gt;

&lt;h3 id=&quot;匹配侧--努力--个性化&quot;&gt;匹配侧 ( 努力 ): 个性化&lt;/h3&gt;

&lt;p&gt;泛化的内容加算法可看性更强，留存也涨;&lt;/p&gt;

&lt;p&gt;用户活跃度的升高，结合进一步泛化内容源 (低门槛创作工具和爆款内容的持续引流，刺激投稿和创作)&lt;/p&gt;

&lt;h3 id=&quot;其它--运气-&quot;&gt;其它 ( 运气 )&lt;/h3&gt;

&lt;p&gt;疫情期间，多出来的宅家线上娱乐时间，也是不可忽视的一大因素&lt;/p&gt;

&lt;p&gt;其它未洞察的点&lt;/p&gt;

&lt;h2 id=&quot;拆分&quot;&gt;拆分&lt;/h2&gt;

&lt;h3 id=&quot;内力&quot;&gt;内力&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;- 内容: 识别爆款、风险，快速进入分发，扩充新热资源
- 供给: 流量及时精准投入
- 消费: 看见新颖、有趣的内容，留存提升
- 匹配: 
    + 用户: 感兴趣且优质内容 + 全方位泛化内容
    + 分析: 回访留存等指标的长期建模
    + 策略: 新内容的审核和爆款引流算法配合
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;如何判定哪些是曝款内容哪些是违规有风险内容，越早的识别爆款可以越早地给予流量，持续爆款新内容流入资源池；从供给端来说，持续的流量也可以让内容生产者的积极性提高，可以持续的从其它平台逐步迁移有生产能力的KOL或KOC；从消费端来说，让用户总能看到新颖、惊喜的内容，从而提升留存。从匹配侧来说，为用户快速找到感兴趣且优质的内容满足当前消费；为用户展现全方位泛化的内容；精细化地进行回访留存等长期建模；新内容的审核与爆款引流算法配合。&lt;/p&gt;

&lt;h3 id=&quot;外力&quot;&gt;外力&lt;/h3&gt;

&lt;p&gt;Local团队对于美国本土化的理解，对于持续内容生态的把控可以带来算法之外的东西；这一部分属于外力，通过外力推动数据的齿轮开始运转，后期就是算法和数据逐步带来良性循环；也可以认为是算法和数据中的专家知识。&lt;/p&gt;

&lt;p&gt;PR、品牌持续曝光，内容生态的布局，为内容的生产和消费做铺垫。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/RS/p1-level.jpg&quot; alt=&quot;多因素影响下的内容分发&quot; /&gt;&lt;/p&gt;

&lt;p&gt;可能抖音走在了飞轮效应那条路上，但是现在只在7或者8这个位置，将来想在美国突破1亿DAU，那需要在接下来的若干发展中继续踩准节奏。&lt;/p&gt;

&lt;h2 id=&quot;数据与算法可以发力的点&quot;&gt;数据与算法可以发力的点&lt;/h2&gt;

&lt;p&gt;借着上面的问题，今天想分享的是，这里面数据和算法可以做什么:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;如何审核出有风险的内容，识别优质的内容；&lt;/li&gt;
  &lt;li&gt;识别出来了优质内容，如何引爆 (算法预测准确率不是百分百，如何及时止损)&lt;/li&gt;
  &lt;li&gt;精准匹配用户和内容，且在最大化短期匹配效率的同时如何兼顾长期留存 (让你嗨，但不能让你太嗨；玩游戏也是一样，太简单和太难都会让留存不好)&lt;/li&gt;
  &lt;li&gt;让有爆款能力的内容生产者，拿到更多的流量，提高其积极性 ( 这里谈论的更多是全站维度 )；&lt;/li&gt;
  &lt;li&gt;如何识别趋势，让虽然不是爆款内容生产者但是满足一定圈层的内容生产者，能够精准地触达其能够覆盖的用户并且获得那个圈层内的流量；&lt;/li&gt;
  &lt;li&gt;泛内容生态下，如何在现有流量体系下做一定干预，给予流量或者限制流量。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;要想变强不能有短板，短板就是你的生命线，有短板的越变越小，有长板的越变越强。&lt;/p&gt;

&lt;h3 id=&quot;如何审核出有风险的内容识别优质的内容&quot;&gt;如何审核出有风险的内容，识别优质的内容&lt;/h3&gt;

&lt;p&gt;相关技术：语音转文本, 标题、内容敏感词识别, 图像识别 ( 黄色、暴力、相似内容 )，新品爆款预测。&lt;/p&gt;

&lt;p&gt;通过上述技术对内容进行初步审核，然后进行小流量测试 ( 其关注的粉丝或内容匹配的类型背后感兴趣的人群 )，如果流量效果表现良好，再通过用户众包的理念对内容进行把控 ( 降低人力审核压力 )，推荐系统分发过程中引入举报途径。&lt;/p&gt;

&lt;p&gt;这里技术和算法的本质是通过算法和数据结合的方式，大幅提升信息密度，在海量内容被创作的时候，审核人力无须每个都做审核，而是对可能违规内容进行审核，如下图。通过人工检测和举报并被确认的违规内容，持续地数据积累，未来机器检测会越来越准。&lt;/p&gt;

&lt;!-- ![内容审核流程](http://localhost:4000/images/RS/p2-audit.png)![分发量和视频分布](http://localhost:4000/images/RS/p3-num-show.png) --&gt;

&lt;center class=&quot;half&quot;&gt;
    &lt;img src=&quot;http://localhost:4000/images/RS/p2-audit.png&quot; width=&quot;60%&quot; /&gt;&lt;img src=&quot;http://localhost:4000/images/RS/p3-num-show.png&quot; width=&quot;40%&quot; /&gt;
&lt;/center&gt;

&lt;p&gt;爆款和优质内容的挖掘至关重要，因为这部分内容承载了平台大部分的播放量，也是吸引用户留存的重要载体。&lt;/p&gt;

&lt;p&gt;产品通过算法和数据也可以一定程度上自动化地挖掘出可能可以引爆的内容点，比如通过类比电商挖掘模式中的种子自动化流程，如下图。&lt;/p&gt;

&lt;center class=&quot;half&quot;&gt;
    &lt;img src=&quot;http://localhost:4000/images/RS/p5-seed.png&quot; width=&quot;60%&quot; /&gt;
&lt;/center&gt;

&lt;!-- ![种子视频筛选](http://localhost:4000/images/RS/p5-seed.png) --&gt;

&lt;h3 id=&quot;识别出来了优质内容如何引爆--算法预测准确率不是百分百如何及时止损-&quot;&gt;识别出来了优质内容，如何引爆 ( 算法预测准确率不是百分百，如何及时止损 )&lt;/h3&gt;

&lt;p&gt;通过新品测试流程，将流量利用最大化，这里可以通过一定概率统计手段进行平滑 ( 同样是20%的点击率，一个曝光是100w，一个是100，后者相对不置信 )，在逐渐积累流量持续进行流量调整；因为你从低点击率上省下来给到了高点击率的商品上，这中间可以通过一些EE的策略或者简单的统计可以完成部分的工作，如下图。&lt;/p&gt;

&lt;center class=&quot;half&quot;&gt;
    &lt;img src=&quot;http://localhost:4000/images/RS/p6-ee.png&quot; width=&quot;50%&quot; /&gt;&lt;img src=&quot;http://localhost:4000/images/RS/p7-ee-1.png&quot; width=&quot;50%&quot; /&gt;
&lt;/center&gt;

&lt;!-- ![流量优化1](http://localhost:4000/images/RS/p6-ee.png)
![流量优化2](http://localhost:4000/images/RS/p7-ee-1.png) --&gt;

&lt;p&gt;这里面也涉及几个问题，就是内容类的产品，不像电商的商品，有很多内容具有较短的生命周期，特别是热点类、时政类的内容，或者优质内容衰退的过程，比如下图。&lt;/p&gt;

&lt;center class=&quot;half&quot;&gt;
    &lt;img src=&quot;http://localhost:4000/images/RS/p8-plr.png&quot; width=&quot;50%&quot; /&gt;&lt;img src=&quot;http://localhost:4000/images/RS/p8-hot.png&quot; width=&quot;50%&quot; /&gt;
&lt;/center&gt;

&lt;!-- ![完播率变化](http://localhost:4000/images/RS/p8-plr.png)
![新热变化](http://localhost:4000/images/RS/p8-hot.png) --&gt;

&lt;p&gt;这部分可能成为爆款快速，但也会快速冷却，所以整套优质候选爆款内容需要持续不断地、快速地被识别出来，并快速获得流量；因为爆款普通内容也存在随着播放量和覆盖人群的上升，完播率及引流转化效率降低的过程（因为没有适合任何人口味的短视频），这时候也需要适时地将这部分流量打到新的爆款上；这些不是不好的内容，是内容已经过了时效或合适人群已经覆盖十之七八了，需要降温。&lt;/p&gt;

&lt;h3 id=&quot;精准匹配用户和内容且在最大化短期匹配效率的同时如何兼顾长期留存--让你嗨但不能让你太嗨玩游戏也是一样太简单和太难都会让留存不好-&quot;&gt;精准匹配用户和内容，且在最大化短期匹配效率的同时如何兼顾长期留存 ( 让你嗨，但不能让你太嗨；玩游戏也是一样，太简单和太难都会让留存不好 )&lt;/h3&gt;

&lt;p&gt;可以借鉴下面的拆解方式。
&lt;img src=&quot;http://localhost:4000/images/RS/p9-theory.png&quot; alt=&quot;流量-转化-反馈&quot; /&gt;&lt;/p&gt;

&lt;p&gt;我们以某天的匹配来看，首先DAU代表了当日的流量，当日我们需要做好匹配 ( 也就是y这个因子 )，其次我们需要更加注重的是x因素，n代表了累计效应。&lt;/p&gt;

&lt;p&gt;换个角度，每日的DAU=新客UV+老客UV，这里面老客UV是前面公式x带来的，在互联网用户天花板明显的情况下，获客成本持续走高，老客维护就是如何在精准匹配的同时，让用户时常回产品看看。&lt;/p&gt;

&lt;p&gt;从交互来看，全屏幕式沉入让用户尽可能沉浸，自动循环播放刺激人的视听，下滑切换让人获得即时满足，不可预测的内容提供间歇性变量奖励，强大的推荐机制个性化快速匹配用户实时需求，让用户上瘾。这中间需对内容和用户进行解构，提取出用户和内容背后的项目特征，然后找到合适的模型进行预估，最终以预估值进行排序和展现。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/RS/p10-rc.png&quot; alt=&quot;个性化匹配&quot; /&gt;
&lt;img src=&quot;http://localhost:4000/images/RS/p11-user-item.png&quot; alt=&quot;算法模型输入&quot; /&gt;&lt;/p&gt;

&lt;p&gt;先说短期匹配效率上的发力点，我们可以通过算法和数据建模，来最大化目标量。流量分发权重可以进行干预，并且不同的干预手段会带来完全不一样的产品最终形态演变。&lt;/p&gt;

&lt;h4 id=&quot;以快手举例&quot;&gt;以快手举例：&lt;/h4&gt;

&lt;p&gt;快手促使主播和粉丝进行更多交互。通过算法推荐，粉丝关注的用户的作品出现在发现页的可能性更高，动态功能让用户有类 似朋友圈的体验，且关注页和个人主页方便通知用户主播发布新作品和开始直播，因此快手和用户的互动更加频繁：快手用户刷关注页的比例高于抖音。未来快手希望将这一比例提升到80%。稠密用户关系网络意味着更高迁移成本。和抖音等侧重内容的平台相比，快手平台的迁移成本更高，用户留存度更高。注：本段以及下面2张图来自峰瑞资本-黄海老师的研究报告/公众号：黄海的消费业观察。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/RS/p12-ks.png&quot; alt=&quot;权重-原则-分布&quot; /&gt;
&lt;img src=&quot;http://localhost:4000/images/RS/p12-ks-2.png&quot; alt=&quot;体验-露出-互动&quot; /&gt;&lt;/p&gt;

&lt;p&gt;所以，相对而言快手希望流量更加扁平，做了一些流量的控制 ( 虽然头部流量还是聚集 )。&lt;/p&gt;

&lt;p&gt;我们以电商来举例，x很大程度上会被平台的商品供应丰富程度、价格、履约/物流速度、后期商品使用、退换货等等后端服务所影响。内容领域相对电商更难具象化，可能是娱乐性、多样性、新颖性、惊喜性等，可以通过对若干指标数据化后，评估长期留存与相关短期指标的关系，最终在推荐或者搜索层面进行干预，比如下图多样性与长期留存的关系。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/RS/p13-rem.png&quot; alt=&quot;留存-追求全局的-不是局部的&quot; /&gt;&lt;/p&gt;

&lt;p&gt;可以尝试在推荐系统中引入一定的多样性控制，可以带来长期指标的提升，尽量将模型带入去全局最优解。这部分的工作也是希望通过兴趣探索在用户某类兴趣衰减明显时，有其它主题内容可以承载用户接下来的时长，比如刚开始用户被漂亮小姐姐的视频吸引，进来以后有时尚、护肤、彩妆、美食、健身、旅行、影视等等其它内容持续地满足他，所以需要在他进来的有限次数中，发现内容平台上更大的世界。&lt;/p&gt;

&lt;h3 id=&quot;让有爆款能力的内容生产者拿到更多的流量提高其积极性--这里谈论的更多是全站维度-&quot;&gt;让有爆款能力的内容生产者，拿到更多的流量，提高其积极性 ( 这里谈论的更多是全站维度 )&lt;/h3&gt;

&lt;p&gt;首先你得对内容生产者进行分层，将平台内的内容生产者进行区分，并根据对平台最终的贡献将他们分层，如下图。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/RS/p13-st.png&quot; alt=&quot;生态&quot; /&gt;&lt;/p&gt;

&lt;p&gt;对平台内容生产者分层后，可以根据其流量配比进行调整，金字塔的底层需要被快速地识别，并将流量减少到可控范围内的最低；前2层需要被鼓励，特别是特色生产者需要流量倾斜，以保证其积极性，对于价值生产者所需流量不够的情况下，可以在非价值生产者内容流量中倾斜一部分。特色内容生产者可以类比第一个主题分享爆款种子内容筛选流程类似，可以通过部分种子特色内容生产者找到更多的特色内容生产者。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/RS/p13-dy.png&quot; alt=&quot;流量分配&quot; /&gt;&lt;/p&gt;

&lt;p&gt;流量如何控制呢，我们可以看下分配流程，当然这个是借鉴淘宝的商品流量分配模式，如上图所示。通过数据化和算法的方式先对整个流量体系进行拆解，将流量拆分成若干主要模块，并通过流量控制系统进行干预，可以实现用户时长、完播率、浏览深度不变的情况下，内容流量分布的调整 ( 至少电商中可以做到gmv不变的情况下，流量实现分配目标的80%-90% )。&lt;/p&gt;

&lt;p&gt;逐步完成几类正反馈：&lt;/p&gt;

&lt;p&gt;初级反馈：点赞数、粉丝数；&lt;/p&gt;

&lt;p&gt;中级反馈：被推荐，获取到了更大的流量；&lt;/p&gt;

&lt;p&gt;高级反馈：对于优秀内容生产者，变现的可能变大。&lt;/p&gt;

&lt;h3 id=&quot;如何识别趋势让虽然不是爆款内容生产者但是满足一定圈层的内容生产者能够精准地触达其能够覆盖的用户并且获得那个圈层内的流量&quot;&gt;如何识别趋势，让虽然不是爆款内容生产者但是满足一定圈层的内容生产者，能够精准地触达其能够覆盖的用户并且获得那个圈层内的流量&lt;/h3&gt;

&lt;p&gt;在推荐系统中，由于算法模型是基于数据的，如果你不做小圈层的区分，很容易较大覆盖率的人群喜好会覆盖小圈层用户的兴趣，因为模型在训练过程中天然就是兼顾大概率的类别的，除非你对建模过程进行干预，比如对小比例的label进行加权等操作。&lt;/p&gt;

&lt;p&gt;南抖音北快手格局的打破，或者快手进攻一、二线城市都是需要突破圈层，逐渐尝试将新圈层的人逐步拉进产品内部；同样B站也是，需要将非二次元爱好者逐步拉入夸大DAU；那如果没有干预或者一些做一些精细化的手段，很难有突破，因为每次引流进来的新圈层用户对老圈层的内容不满意，最终离开 ( 周杰伦入驻快手，前期进来的一、二线城市的用户留存好，过半个月基本留存就惨不忍睹了；抖音也有进攻快手腹地，但是留存差的情况 )；这也是一般用户增长团队的一个非常重要的事情。&lt;/p&gt;

&lt;p&gt;破圈层有三个事情要做，第一通过产品目标定位清楚需要扩的人群，或者通过站内数据分析发现潜力群体，第二通过深入地挖掘找到他们的一些兴趣点和话题，并开始进行内容延展，第三内容体系够完善加上更精准的分群推荐及体验的区隔，最终可以培养产品内新圈层的人群，只有这群人到达一定体量，接下来就是数据和算法的事情了，他们会通过内容和人群算法为他们带来个性化的体验。这里给一篇小红书在做15岁以下小学和初中生的留存的案例：&lt;/p&gt;

&lt;p&gt;https://zhuanlan.zhihu.com/p/58241575&lt;/p&gt;

&lt;p&gt;新圈层的需求如何发现呢，上述文章中有几种方案，一种方案是分析现有产品体系下某些关键指标低的群体，然后根据他们站内的行为，配合这批用户进来的初期，进行冷启动或者适合内容的筛选与补充。在很多年前你做产品，你的种子用户决定了你产品未来；现在这个阶段也是，只不过这个阶段，对于需要扩圈的产品来说，他们需要时不时地去维护新进来圈层的种子用户，并让他们消费和生产更多这个圈层喜欢的内容，带来圈层的扩大。&lt;/p&gt;

&lt;p&gt;如何洞察呢？可以通过站内数据，比如搜索词、搜索主题环比变化情况 ( 见下图 )，用户消费内容效率的环比情况；分群再看上述指标的情况；&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/RS/p13-query.png&quot; alt=&quot;洞察&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;泛内容生态下如何在现有流量体系下做一定干预给予流量或者限制流量&quot;&gt;泛内容生态下，如何在现有流量体系下做一定干预，给予流量或者限制流量&lt;/h3&gt;

&lt;p&gt;这一块类似商品的新内容体系方案，需要强制在流量侧给予倾斜，对新类型内容进行扶持；当然这里面其实也可以做到用数据和算法驱动，提升效率。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/RS/p13-cy.png&quot; alt=&quot;创新机会发现&quot; /&gt;
&lt;img src=&quot;http://localhost:4000/images/RS/p12-td.png&quot; alt=&quot;权衡&quot; /&gt;
&lt;img src=&quot;http://localhost:4000/images/RS/p12-td-2.png&quot; alt=&quot;取舍&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;总结&quot;&gt;总结&lt;/h2&gt;

&lt;p&gt;数据和算法是0，初期的冷启动和运营是1，大于1还是小于1很关键，这个考验的就是创始人团队的能力；如果小于1，后面加0都是无用功，如果大于1，后面加零就是快速地增长。比如我所从事的电商领域，不只是前台的流量精准匹配 ( 推荐、搜索 ) 做好就可以了，你还需要有优质的货品，极快极好的履约 ( 物流、退换货 ) 等等，算法和数据只是其中一块，但是是产品成长路上重要的一块。&lt;/p&gt;

&lt;p&gt;如何系统性地构建数据与算法体系，并跟其他构建系统的模块手拉手，构建飞轮效应，逐渐扩圈是接下来互联网产品持续增长的动力。&lt;/p&gt;

&lt;p&gt;## 参考&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;[1] &lt;a href=&quot;https://mp.weixin.qq.com/s/QMORttE80yiIQnkVQFLxUQ&quot;&gt;TikTok抖音国际版留存背后的数据和算法推演&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;[2] &lt;a href=&quot;https://en.wikipedia.org/wiki/Main_Page&quot;&gt;Wikipedia&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sun, 15 Mar 2020 00:00:00 +0800</pubDate>
        <link>http://localhost:4000//2020/03/feed-content-recommand-system.html</link>
        <guid isPermaLink="true">http://localhost:4000//2020/03/feed-content-recommand-system.html</guid>
        
        
      </item>
    
      <item>
        <title>短视频消费的若干分析和总结</title>
        <description>&lt;p&gt;2018年，美团ceo王兴说：2019年可能会是过去十年里最差的一年,但却是未来十年里最好的一年。仅以2020年的开头看，这句话差不多就对了大半。资本寒冬，经济下行，而且还难以看到一个增长点。我身边的朋友普遍反应，生意不好做。这个时候，也是最考验企业和个人的时候。而往往在这个时期成长起来的公司和产品，很多恰恰是下一个时代的标杆。&lt;/p&gt;

&lt;p&gt;下一个10年才刚刚开始，但不管会发生怎样的变化，短视频很大程度上已经拿到一张标杆的门票。以一个15秒的平快视频，搭载动感十足的音乐，成功抓住了无数老铁的芳心。从一线城市的地铁、写字楼，到五环外的农村集市，目光所及，几乎都是上滑下拉。&lt;/p&gt;

&lt;p&gt;“为什么短视频会火？抖音和快手是如何做到的？”&lt;/p&gt;

&lt;p&gt;一个产品不火，可能有无数的理由；但是能够火的产品，却只可能有一个原因：满足用户的需求。 一句正确的废话。互联网产品的划分思路，以王兴的三横四纵理论为著：
纵向代表技术变革方向，横向代表用户需求的发展方向。&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;item&lt;/th&gt;
      &lt;th&gt;资讯&lt;/th&gt;
      &lt;th&gt;交流&lt;/th&gt;
      &lt;th&gt;娱乐&lt;/th&gt;
      &lt;th&gt;商务&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Web 1.0 搜索&lt;/td&gt;
      &lt;td&gt;Yahoo/Google/Baidu&lt;/td&gt;
      &lt;td&gt;MSN/QQ&lt;/td&gt;
      &lt;td&gt;百度音乐/盛大游戏&lt;/td&gt;
      &lt;td&gt;阿里/淘宝&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Web 2.0 社交&lt;/td&gt;
      &lt;td&gt;Twitter/微博&lt;/td&gt;
      &lt;td&gt;FaceBook/人人网&lt;/td&gt;
      &lt;td&gt;开心网&lt;/td&gt;
      &lt;td&gt;美丽说/蘑菇街/互联网金融&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Web 3.0 移动&lt;/td&gt;
      &lt;td&gt;今日头条&lt;/td&gt;
      &lt;td&gt;微信&lt;/td&gt;
      &lt;td&gt;手游&lt;/td&gt;
      &lt;td&gt;美团点评、穿戴式设备&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;纵轴是和时间相关的变量，技术变更代表着新的方式和手段，从而带来产品上的升级和行业的兴盛。横向是相对稳定的，并具有驱动性。用户的需求驱动着技术的发展，技术的发展影响着需求的满足。需求是确定并存在于当前的，技术的升级却是可遇而不可求的。所以本文主要聚焦在用户，从人的需求角度上，进行产品划分。&lt;/p&gt;

&lt;p&gt;提起需求分析，就绕不开亚伯拉罕·马斯洛，他于1943年提出的需求层次理论: 人的需求从低到高依次分为生理需求、安全需求、社交需求、尊重需求和自我实现需求。按照马前辈的说法，这5个需求的层次是逐步推进的，形成一个正金子塔结构，人们更应该向着更高的需求发展。这个看起来很对，但其实也不完全对。从感性角度讲，金字塔越往上，获得的自由和满足越多，也可以说人活得更加幸福，看似需求也变得更难以满足。&lt;/p&gt;

&lt;p&gt;其实不是的。如果把金字塔倒过来看：“自我实现”是一种个人认同感，实现成本只在个人一念之间，”尊重需求“是一种感受，很大程度上也取决于个人的态度，”社交需求“是一种形式手段，不能说是一个需求，”安全需求“是要物质和感受共同保证的，”生理需求“是需要切实的物质和资源来满足的。经过这样的解析，其实最终的需求，并不在顶层，而在底部。但受制于环境和资源，这是不可能被完全满足的部分，却也是人最需要被满足的部分。这也能解释，虽然各国政府和司法对黄毒赌暴等犯罪行为，处以很大的惩罚，但是这些灰色地带依然还存在。&lt;/p&gt;

&lt;p&gt;经过上述的解析，可以把用户的需求大致可以分成：衣食住行、社交、工作、工具；&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;衣食住行&lt;/th&gt;
      &lt;th&gt;社交&lt;/th&gt;
      &lt;th&gt;工作&lt;/th&gt;
      &lt;th&gt;工具&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;淘宝、京东；美团、口碑；自如、链家；滴滴、曹操&lt;/td&gt;
      &lt;td&gt;微信、陌陌、FaceBook&lt;/td&gt;
      &lt;td&gt;脉脉、Boss&lt;/td&gt;
      &lt;td&gt;360杀毒、Office软件&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;按产品领域来拆分，工具类产品很直接，比如，360杀毒是用于电脑杀毒的，uc浏览器是用来浏览网页的。服务类产品，也能大致这算成服务需求，如：美团，提供外卖、酒率&lt;/p&gt;
</description>
        <pubDate>Sun, 23 Feb 2020 00:00:00 +0800</pubDate>
        <link>http://localhost:4000//2020/02/How-to-Shoot-a-Nice-Video.html</link>
        <guid isPermaLink="true">http://localhost:4000//2020/02/How-to-Shoot-a-Nice-Video.html</guid>
        
        <category>产品分析</category>
        
        
      </item>
    
      <item>
        <title>Wide &amp; Deep 系列CTR预估论文</title>
        <description>&lt;h3 id=&quot;wide--deep-learning-for-recommender-systems&quot;&gt;Wide &amp;amp; Deep Learning for Recommender Systems&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;wide linear models:  $\underline{memorization}$&lt;/li&gt;
  &lt;li&gt;deep neural networks: $\underline{generalization}$&lt;/li&gt;
  &lt;li&gt;Vocabularies, which are tables mapping categorical feature strings to integer IDs, are also generated in this stage.
The system computes the ID space for all the string features
that occurred more than a minimum number of times. Continuous real-valued features are normalized to $[0, 1]$ by mapping a feature value x to its cumulative distribution function
$P(X \leq x)$, divided into $n_q$ quantiles. The normalized value
is $\frac{i−1}{n_q-1}$ for values in the i-th quantiles. 
&lt;img src=&quot;http://localhost:4000/images/paper/deepxxxx/wide-deep.png&quot; alt=&quot;wide&amp;amp;deep&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;deepfm-a-factorization-machine-based-neural-network-for-ctr-prediction&quot;&gt;DeepFM: A Factorization-Machine based Neural Network for CTR Prediction&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;这个是 wide_deep 的升级版本，主要贡献在于引入 FM 解决稀疏特征的二阶交叉的问题，减少特征工程的工作量。而且FM的时间复杂度是 $O(nk)$, $k$ 是超参数，一般很小。&lt;/li&gt;
  &lt;li&gt;引入 embedding layer, 使用joint learning 的方式进行训练，实现end-to-end&lt;/li&gt;
  &lt;li&gt;缺点：
    &lt;ul&gt;
      &lt;li&gt;所有特征项统一进行embedding 成相同的维度，不能差异化体现不同特征的贡献&lt;/li&gt;
      &lt;li&gt;数值型特征需要先转化成类别型特征，才能处理&lt;/li&gt;
      &lt;li&gt;同一特征与不同特征的交叉，使用相同的权重embedding，过于粗暴&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;###&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;通过和一阶项进行逐步交叉，得到多阶项。同时避免高阶项之间的交叉，从而控制了参数数量。&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;http://localhost:4000/images/paper/deepxxxx/wide-cross.png&quot; alt=&quot;wide&amp;amp;deep&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Sun, 26 Jan 2020 00:00:00 +0800</pubDate>
        <link>http://localhost:4000//2020/01/Wide-Deep-Series-CTR-Prediction.html</link>
        <guid isPermaLink="true">http://localhost:4000//2020/01/Wide-Deep-Series-CTR-Prediction.html</guid>
        
        <category>wide&amp;deep</category>
        
        <category>DeepFM</category>
        
        
      </item>
    
      <item>
        <title>悲欣交集，热泪盈眶</title>
        <description>&lt;p&gt;几番折腾，我还是离职了。说不清是该开心，还是应该失落。从来这里开始，就一直想走，一直在告诫自己，我不要属于这里，我应该更好的。而这段难以解释的经历，也躺在简历上，走过了一个又一个面试。期间，有过的屈辱感、不甘心、却又是无可奈何；我一度怀疑人生，到一种不可理解的程度。为什么总是我？为什么我要这么悲剧。&lt;/p&gt;

&lt;p&gt;人总是很容易放过自己，把责任和问题，都推向别人或者是环境。我在初期的时候，就是这样，总觉得坑。愤愤不平。被安排的工作内容和方向，是有些不match的，但也不至于是完全不沾边。遇到问题，“抱怨”相比“应对”，更多一些。而我的解决方案，也很粗暴：再选一次。现在想想，甚至觉得有些庆幸：幸亏他们没有给我发offer. 因为，我需要变得更好，我值得更好的。&lt;/p&gt;

&lt;p&gt;屡次碰壁后，也算是吸取了教训。先熬着干吧。机遇也随之而来，空降了一个老大，重新调整了项目和业务。我被分配了一个小项目，由我来主导。偏探索和创新：往好的想，这是机遇；往坏的看，这又是个坑。一直到我离开，我问我的老大，当时立项的时候，是怎么看的。他说，原来的定位，就是一个功能和体验提升，真没指望能带来很大的收益。这当然很对，也符合我接手时的感受。但最终，这个项目不仅做成了，而且做出了很大的突破。项目推全后，直接带动了整体大盘的总媒体时长的显著提升，算是半年以来，feed短视频分发中，举足轻重的算法和产品创新之一。&lt;/p&gt;

&lt;p&gt;做完这个项目后，手上又幸运地拿到几个offer。我重新面临了选择，也因此我得以认真的思考，并记下这半年来的点点滴滴。&lt;/p&gt;

&lt;h3 id=&quot;是选择重要还是努力重要&quot;&gt;是选择重要？还是努力重要？&lt;/h3&gt;

&lt;p&gt;“选择比努力重要”是一句近乎真理的鸡汤，大行其道。但我在一次面试中，有个高级面试官给我讲，“努力比选择更重要”，因为只有踏踏实实的努力，才能有沉淀；“选择”过多，导致根底浅。我很认同他的观点，虽然他没有给我面试通过。我依然很感激。他以他切身的工作体会，启发了我很多。最优解，往往是看起来最笨的解，坚持而已。&lt;/p&gt;

&lt;p&gt;我认同他，但并代表他就是完全对的，每个人的经历都有所不同，需要具体情况，具体看待。&lt;/p&gt;

&lt;p&gt;选择很重要，拥有选择的能力更加重要。每一次选择，都是一次消耗，也都是一种开始，孕育无限可能。是因为你的选择，指引了你的努力；而你努力的结果，又决定了你能否选择。不能为了选择而选择，好比不要为了逃避问题而跳槽，那样只会是不断地跳槽，并没有什么质的改变。要认真的思考，自己的核心竞争力是什么，绝对不是一个title，也绝对不是一个大公司平台。&lt;/p&gt;

&lt;p&gt;是选择重要，还是努力重要？这就是一个蛋生鸡，鸡生蛋的问题。他们都很重要。但是，从根源上说：拥有选择，才是最本质、最纯粹的。&lt;/p&gt;

&lt;p&gt;你的努力，成就你的实力；因为实力，才有选择。&lt;/p&gt;

&lt;p&gt;我很庆幸，我当时听进去那个面试官的建议了。所以后来，我以一个owner的心态，用心做手上的项目，逐版本迭代，一步步提升，并最终为团队和自己赢得了尊重，并为自己赢得了选择。&lt;/p&gt;

&lt;h3 id=&quot;是智商重要还是情商重要&quot;&gt;是智商重要？还是情商重要？&lt;/h3&gt;

&lt;p&gt;这个问题等价于，是做事重要，还是处关系重要。两者都很重要，但两者在不同的情形下有不同的重要程度。做事，是立身之本。我们通过做事来证明自己，又因为证明自己，需要和别人打交道，处好关系是很重要的。&lt;/p&gt;

&lt;p&gt;职场上，我的理解是：做事是目标，处关系是辅助。关系，可能并没有那么重要。情商，是重要的，但是这不应该成为绝大多时候，绝大多事情的决定因素，如果是，那此地不宜久留。真诚是最高的情商。用心、真诚、用功，是最好的名片。用结果说话，真诚待人。&lt;/p&gt;

&lt;h3 id=&quot;感恩的心&quot;&gt;感恩的心&lt;/h3&gt;

&lt;p&gt;懂得感恩，是一个人真正成熟的标志。人总是容易把自己看大，把功劳往自己身上算。其实一件事能成，是由很多的点共同决定的，孰重孰轻，很难说。比如，是屁眼重要，还是脑袋重要。脑袋里有思考，有智慧，是身体的中枢灵魂，当然重要。屁眼只负责拉屎，不重要。可是屁眼要是罢工，这人就废了。&lt;/p&gt;

&lt;p&gt;大脑是重要的，但是屁眼也是有用的。价值有高低，但是地位是平等的，都是不可或缺的一部分。要有一个包容和理解的心，感恩的心。就像部门里做事，有些人就像屁眼一样，故意憋着不支持，让人很难受。但不能因为这一些点，就完全否定了他们的作用。&lt;/p&gt;

&lt;p&gt;作为项目的owner，既要有主权意识，要在项目中注入自己的设计思想和理念。同时，也要虚心听取多方的意见。对于同战壕的小伙伴，真诚相待，用心把事做成。心怀感恩。如果没有大家支持和理解，是很难把事情做成的；中间有好多把，我在进度和反馈上，都出现了问题，是大家的包容和耐心，一起孵化了这个项目。&lt;/p&gt;

&lt;p&gt;要知道，有很多边边角角的事情，有很多繁琐的细节，有很多你不曾看见的付出，也是项目中很重要的环节。要感谢他们，并心怀感激。我说的是真心的感谢，不是客气。&lt;/p&gt;

&lt;p&gt;要用项目的结果和收益，来表示实际的感恩。用实际的效益，报答和“报复”，那些支持和“支持过”你的人。不要用“伤害”对付“伤害”，用自强和感恩。&lt;/p&gt;

&lt;p&gt;你若芬芳，花香自来。&lt;/p&gt;

&lt;h3 id=&quot;最终为什么还是决定走&quot;&gt;最终为什么还是决定走？&lt;/h3&gt;

&lt;p&gt;短视频追剧项目做成后，我很爽地工作了很长时间，而且只要我愿意，我可以一直这样爽下去。因为有前面的铺垫，后面我只需要从两个点上发力：拓展业务边界，迭代现有机制。前者可以推动内容理解团队，抽取资源放到词典里；后者，在现有框架下调参训练。都是很业务性的工作。说得好听点是这样，说白了，就是自己看看剧，体验下，并结合着看剧体验，自己调整下策略和算法。&lt;/p&gt;

&lt;p&gt;项目从接手到推全，我花了差不多快4个月，其中很大的精力都放在了规划和设计上（策略参数化配置，系统模块化分离，线上和线下业务并行，功能和指标优化分级等），事实上这非常值得，虽然前期有很长时间没有产出，但是这套体系完成后，我可以很轻松地进行策略的调整、升级和优化，在出现bad case 时轻松定位问题，快速进行各个维度上的数据统计分析。可以将自己从代码中抽出身来，用更多的时间进行算法和产品上的思考。我曾开玩笑的说，我现在的工作就是看剧，拿收益。当然，成长很大，我学习到了，即时兴趣点的捕捉，资源的筛选，用户体验的评估和刻画，还有和不同身份的人有效的沟通，出现冲突时，如何应对，该坚持什么，该放下什么。是的，我做到了。&lt;/p&gt;

&lt;p&gt;我有一段时间很迷恋于这个设计体系，过得真的很舒服。上午10点多到公司，看看数据监控和报表，查看下邮件和今天的新需求；中午吃个饭，溜溜弯；午休完，开始看看剧，想想哪块可以再优化下；下午开会，然后对应着改点配置文件；晚上继续看剧，写个日报，愉快回家。我一度把这种生活，当作之前辛苦付出的回报。我就应该享有这样的工作节奏，在一个组都忙得要命的时候，一个人悠闲地看看剧，追追case，但是产出却是很好。&lt;/p&gt;

&lt;p&gt;我很享受这个舒服。但是期间也有很难受的点。传话筒式的PM、M，让人真的太难受了。没有对业务的思考，也没有主见，却很强势地催着进度。彷佛，只要你忙着，就可以，即使是无用的瞎忙。对不起，我拒绝。这差不多能算是我准备走的导火索，重新将“再选一次”提上日程。当然，“效果”和“收益”是最好的武器，我可以毫不留情地怼掉需求，怼不掉的，我就拖着。用正确有效的策略，去提升、去优化。依然可以很舒服。到后来，反倒是我自己不愿意走了，我很抗拒去重新开始，很害怕从头再来。这种心态直白的说，享受安逸。&lt;/p&gt;

&lt;p&gt;做正确的事，永远比做舒服的事，重要得多。&lt;/p&gt;

&lt;p&gt;最终，我还是选择了走。我能看到的是，目前这个项目有价值、有成长的部分，已经过去了。而且，在这里重新去推一个项目，也只是重复这个思路。重要的是，以一己之力，去抗衡整个的M、PM体系，是非常没必要的。我不能保证，每一次我的想法和尝试都是正确的，每一次迭代都有效果，而这里，对错误的包容性并不大。容易被标签化。到时候，反倒会特别不愉快。&lt;/p&gt;

&lt;p&gt;这倒不是说这些同事坏，他们也有他们的压力，每个人都只是在履行自己的职责，每个人都很敬业。只不过，这个大熊公司体系下，每个人都不开心，一级压一级，平级推平级，相互折磨。&lt;/p&gt;

&lt;p&gt;“谁受益，谁推动，解决不了，escalate” 说这个slogan不是智商下线了，就是没安好心。如果项目的推进完全靠受益者来驱动，那肯定做不成的。因为，项目的收益，不可能让每个付出者都能受益。点点滴滴的默默付出者，也是项目进行中的重要组成部分。这个明显的slogan只会进一步恶化现有的同事关系，让项目更加难以推动。我甚至认为，这个slogan宣示着这个公司的末日，不远了。有些问题是局部的、暂时的，而这种一号位的问题，是结构性的、本质的，这个是要命的。&lt;/p&gt;

&lt;p&gt;更荒唐的是，厂长开始召回老员工，当然他们有很光鲜的履历和title，似乎也匹配，正是他们成就了现有的辉煌。厂长是不是觉得，他们回来，就能回到当初，回到那种状态。”哪个能打的XX又回来啦“。这是非常幼稚的管理思想。环境已经变了，却还想通过召回不变的人，来起死回生，何其扯淡。何况人也是变了。更何况，有些人是因为自身的实力，而有些人只是因为时机和平台。乌烟瘴气的OKR，城下之盟的对齐，只会堆砌更多的浮华，吹出更致命的泡沫。船会沉的。&lt;/p&gt;

&lt;p&gt;只有从心里想把事情做成的人，才可能做成事；是机遇和平台，成就了很多的人；但不是每个厉害的人，都能成就平台。对不起，留给XX的时间不多了。&lt;/p&gt;

&lt;p&gt;于我，停在这个阶段和位置，只是成为这个体系下的一颗毒瘤。只有放下昔日的光辉，才能不断前进。我的核心竞争力，应该是“把事做成”，而不仅仅是这安逸的收获。&lt;/p&gt;

&lt;p&gt;年轻，是一种选择未知的勇气。永远年轻，永远热泪盈眶。#&lt;/p&gt;

</description>
        <pubDate>Tue, 02 Jul 2019 00:00:00 +0800</pubDate>
        <link>http://localhost:4000//2019/07/good-bye-baidu-feed.html</link>
        <guid isPermaLink="true">http://localhost:4000//2019/07/good-bye-baidu-feed.html</guid>
        
        <category>work</category>
        
        
      </item>
    
      <item>
        <title>熵和机器学习</title>
        <description>&lt;h3 id=&quot;一自信息&quot;&gt;一、自信息&lt;/h3&gt;
&lt;p&gt;自信息表示某一事件发生时所带来的信息量的多少，当事件发生的概率越大，则自信息越小，或者可以这样理解：某一事件发生的概率非常小，但是实际上却发生了(观察结果)，则此时的自信息非常大；某一事件发生的概率非常大，并且实际上也发生了，则此时的自信息较小。&lt;/p&gt;

&lt;p&gt;\begin{equation}
I(p_i) = -log(p_i)
\end{equation}&lt;/p&gt;

&lt;h3 id=&quot;二信息熵&quot;&gt;二、信息熵&lt;/h3&gt;
&lt;p&gt;自信息描述的是随机变量的某个事件发生所带来的信息量，而信息熵通常用来描述整个随机分布所带来的信息量平均值，更具统计特性。&lt;/p&gt;

&lt;p&gt;\begin{equation}
H(X) = E_{x\sim p}[I(x)] = -E_{x\sim p}[log(p(x))] 
= -\sum_{i=1}^{n}p(x_i)log(p(x_i))
= -\int_{x}p(x)log(p(x))dx
\end{equation}&lt;/p&gt;

&lt;p&gt;从公式可以看出，信息熵H(X)是各项自信息的累加值，由于每一项都是整正数，故而 $\underline{随机变量取值个数越多，状态数也就越多，累加次数就越多，信息熵就越大，混乱程度就越大，纯度越小.}$ 越宽广的分布，熵就越大，在同样的定义域内，由于分布宽广性中脉冲分布&amp;lt;高斯分布&amp;lt;均匀分布，故而熵的关系为脉冲分布信息熵&amp;lt;高斯分布信息熵&amp;lt;均匀分布信息熵。可以通过数学证明，当随机变量分布为均匀分布时即状态数最多时，熵最大。熵代表了随机分布的混乱程度，这一特性是所有基于熵的机器学习算法的核心思想。&lt;/p&gt;

&lt;p&gt;多维联合信息熵：
\begin{equation}
H(X, Y) = -\sum^n_{i=1}\sum^m_{j=1}p(x_i,y_j)log(p(x_i,y_j))
\end{equation}&lt;/p&gt;

&lt;h3 id=&quot;三条件熵&quot;&gt;三、条件熵&lt;/h3&gt;
&lt;p&gt;条件熵的定义为：在X给定条件下，Y的条件概率分布的熵对X的数学期望
\begin{equation}
H(Y|X)  = E_{x\sim p}[H(Y|X=x)] = \sum^n_{i=1} p(x)H(Y|X = x) 
 = - \sum^n_{i=1}p(x)\sum^m_{j=1}p(y|x)log(p(y|x)) 
 = -\sum^n_{i=1}\sum^m_{j=1}p(x,y)log(y|x) 
\end{equation}&lt;/p&gt;

&lt;p&gt;\begin{equation}
= -\left(\sum^n_{i=1}\sum^m_{j=1}p(x,y)log(p(x,y)) - \sum^n_{i=1}\sum^{m}_{j=1}p(x,y)log(p(x))\right)
\end{equation}&lt;/p&gt;

&lt;p&gt;\begin{equation}
= -\left(\sum^n_{i=1}\sum^m_{j=1}p(x,y)log(p(x,y)) - \sum^n_{i=1}log(p(x))\sum^m_{j=1}p(x,y)\right)
\end{equation}&lt;/p&gt;

&lt;p&gt;\begin{equation}
= -\left(\sum^n_{i=1}\sum^m_{j=1}p(x,y)log(p(x,y)) - \sum^n_{i=1}log(p(x))p(x)\right)
= H(X,Y) - H(X)
\end{equation}&lt;/p&gt;

&lt;h3 id=&quot;四交叉熵&quot;&gt;四、交叉熵&lt;/h3&gt;
&lt;p&gt;主要用于度量两个概率分布间的差异性信息。p对q的交叉熵表示q分布的自信息对p分布的期望，公式定义为：
\begin{equation}
H(p;q) = E_{x\sim p}[-log(q(x))] = -\sum^n_{i=1}p(x)log(q(x))
\end{equation}
其中。p是真实样本分布，q是预测得到样本分布。在信息论中，其计算的数值表示：如果用错误的编码方式q去编码真实分布p的事件，需要多少bit数，是一种非常有用的衡量概率分布相似性的数学工具。&lt;/p&gt;

&lt;p&gt;在机器学习中，经常用到的交叉熵：
\begin{equation}
J(\theta) = -\frac{1}{m}\sum^m_{i=1}\left(y_i log\, h_{\theta}(x_i) + (1-y_i)log\,(1-h_{\theta}(x_i))\right)
\end{equation}&lt;/p&gt;

&lt;h3 id=&quot;五相对熵kl散度&quot;&gt;五、相对熵（KL散度）&lt;/h3&gt;
&lt;p&gt;$\textbf{交叉熵，其用来衡量在给定的真实分布下，使用非真实分布所指定的策略消除系统的不确定性所需要付出的努力的大小;}$ 
$\textbf{相对熵，其用来衡量两个取值为正的函数或概率分布之间的差异}$&lt;/p&gt;

&lt;p&gt;\begin{equation}
\begin{array}{l}{\qquad \begin{array}{c}{D_{K L}(p | q)=E_{x \sim p}\left[\log \frac{p(x)}{q(x)}\right]=-E_{x \sim p}\left[\log \frac{q(x)}{p(x)}\right]} \ 
{=-\sum_{i=1}^{n} p(x) \log \frac{q(x)}{p(x)}=H(p; q)-H(p)}\end{array}}\end{array}
\end{equation}
简单对比交叉熵和相对熵，可以发现仅仅差了一个H(p)，如果从优化角度来看，p是真实分布，是固定值，最小化KL散度情况下，H(p)可以省略，此时交叉熵等价于KL散度。&lt;/p&gt;

&lt;p&gt;在最优化问题中，最小化相对熵等价于最小化交叉熵；相对熵和交叉熵的定义其实都可以从最大似然估计得到，下面进行详细推导:以某个生成模型算法为例，假设是生成对抗网络GAN，其实只要是生成模型，都满足以下推导。&lt;/p&gt;

&lt;p&gt;若给定一个样本数据的真实分布 $P_{data}(x)$ 和生成的数据分布 $P_{G}(x;\theta)$, 那么生成模型希望找到一组参数 $\theta$ 使得分布 $P_{G}(x;\theta)$ 和 $P_{data}(x)$之间的距离最短。也就是找到一组生成器参数，使得生成器能生成十分逼真的分布。现在从真实分布 $P_{data}(x)$ 中抽取m个真实样本： &lt;script type=&quot;math/tex&quot;&gt;x^{1}, x^{2}, \ldots x^{m}&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\underline{对于每一个真实样本，我们可以计算 P_{G}(x^i;\theta), 即在由 \theta 确定的生成分布中，x^i 样本所出现的概率。}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;因此， 我们可以构建似然函数：
\begin{equation}
L=\prod_{i=1}^{m} P_{G}\left(x^{i} ; \theta\right)
\end{equation}&lt;/p&gt;

&lt;p&gt;最大化似然函数，即可求得最优参数 $\theta^*$, 即：
\begin{equation}
\theta^*=arg \max_{\theta} \prod_{i=1}^{m} P_{G}\left(x^{i} ; \theta\right)
\end{equation}&lt;/p&gt;

&lt;p&gt;转化成对数似然函数：
\begin{equation}
\theta^{*}=\arg {\max_{\theta}} \log \prod_{i=1}^{m} P_{G}\left(x^{i} ; \theta\right)
\end{equation}&lt;/p&gt;

&lt;p&gt;\begin{equation}
= \arg \max_{\theta} \sum_{i=1}^{m} \log P_{G}\left(x^{i} ; \theta\right)
\end{equation}&lt;/p&gt;

&lt;p&gt;由于是求最大值，故整体乘上常数对结果没有影响,这里是逐点乘上一个常数，所以不能取等于号，但是因为在取得最大值时候 $P_{G}\left(x^{i} ; \theta\right)$ 和 $P_{data}\left(x\right)$肯定是相似的，并且肯定大于0，所以依然可以认为是近视相等的:
\begin{equation}
\approx \arg \max_{\theta} \sum_{i=1}^{m} P_{d a t a}\left(x^{i}\right) \log P_{G}\left(x^{i} ; \theta\right)
\end{equation}
\begin{equation}
=\arg \max_{\theta} E_{x_i \sim p_{\text { data }}}\left[\log P_{G}\left(x^{i} ; \theta\right)\right]
\end{equation}&lt;/p&gt;

&lt;p&gt;上面这个公式正好是交叉熵的定义式，我们在这个基础上减去一个常数项：&lt;/p&gt;

&lt;p&gt;\begin{equation}
\theta^{*}=\arg \max \{ E_{x \sim p_{\text {data}}}\left[\log P_{G}\left(x^{i} ; \theta\right)\right] - E_{x \sim p_{\text {data}}}\left[\log P_{\text {data}}\left(x^{i}\right)\right] \}
\end{equation}&lt;/p&gt;

&lt;p&gt;\begin{equation}
=\arg \max_{\theta} \int_{x} P_{data} \log \frac{P_{G}(\theta)}{P_{data}} dx
\end{equation}&lt;/p&gt;

&lt;p&gt;\begin{equation}
=\arg \min_{\theta} \int_{x} P_{data} \log \frac{P_{data}}{P_{G}(\theta)} dx
\end{equation}&lt;/p&gt;

&lt;p&gt;\begin{equation}
=\arg \min_{\theta} E_{x \sim P_{data}}\left[\log \frac{P_{data}}{P_{G}(\theta)}\right]
\end{equation}&lt;/p&gt;

&lt;p&gt;\begin{equation}
=\arg \min_{\theta} K L\left(P_{data}(x) || P_{G}(x ; \theta)\right)
\end{equation}&lt;/p&gt;

&lt;p&gt;$\underline{最大化似然函数，等价于最小化负对数似然，等价于最小化交叉熵，等价于最小化KL散度}$&lt;/p&gt;

&lt;h3 id=&quot;六互信息&quot;&gt;六、互信息&lt;/h3&gt;
&lt;p&gt;互信息在信息论和机器学习中非常重要，其可以评价两个分布之间的距离，这主要归因于其$\underline{对称性}$，假设互信息不具备对称性，那么就不能作为距离度量，例如相对熵，由于不满足对称性，故通常说相对熵是评价分布的相似程度，而不会说距离。互信息的定义为：一个随机变量由于已知另一个随机变量而减少的不确定性，或者说从贝叶斯角度考虑，由于新的观测数据y到来而导致x分布的不确定性下降程度。公式如下：
\begin{equation}
\begin{aligned} I(X, Y) =H(X)-H(X | Y)=H(Y)-H(Y | X) =H(X)+H(Y)-H(X, Y) = H(X, Y)-H(X | Y)-H(Y | X) \end{aligned}
\end{equation}&lt;/p&gt;

&lt;p&gt;互信息和相对熵也存在联系，如果说相对熵不能作为距离度量，是因为其非对称性，那么互信息的出现正好弥补了该缺陷，使得我们可以计算任意两个随机变量之间的距离，或者说两个随机变量分布之间的相关性、独立性。
\begin{equation}
I(X, Y)=K L(p(x,y) || p(x) p(y) )
\end{equation}
互信息也是大于等于0的，当且仅当x与y相互独立时候取等号。&lt;/p&gt;

&lt;h3 id=&quot;七信息增益&quot;&gt;七、信息增益&lt;/h3&gt;
&lt;p&gt;\begin{equation}
g(D, A)=H(D)-H(D | A)
\end{equation}
其中D表示数据集，A表示特征，信息增益表示得到A的信息而使得类X的不确定度下降的程度，在ID3中，需要选择一个A使得信息增益最大，这样可以使得分类系统进行快速决策。&lt;/p&gt;

&lt;h3 id=&quot;八信息增益率&quot;&gt;八、信息增益率&lt;/h3&gt;
&lt;p&gt;信息增益率是决策树C4.5算法引入的划分特征准则，其主要是克服信息增益存在的在某种特征上分类特征细，但实际上无意义取值时候导致的决策树划分特征失误的问题。例如假设有一列特征是身份证ID，每个人的都不一样，其信息增益肯定是最大的，但是对于一个情感分类系统来说，这个特征是没有意义的，此时如果采用ID3算法就会出现失误，而C4.5正好克服了该问题。其公式如下：&lt;/p&gt;

&lt;p&gt;\begin{equation}
g_{r}(D, A)=g(D, A) / H(A)
\end{equation}&lt;/p&gt;

&lt;h3 id=&quot;九基尼系数&quot;&gt;九、基尼系数&lt;/h3&gt;
&lt;p&gt;基尼系数是决策树CART算法引入的划分特征准则，其提出的目的不是为了克服上面算法存在的问题，而主要考虑的是计算快速性、高效性，这种性质使得CART二叉树的生成非常高效。其公式如下：&lt;/p&gt;

&lt;p&gt;\begin{equation}
\begin{aligned} \operatorname{Gini}(p)=&amp;amp; \sum_{i=1}^{m} p_{i}\left(1-p_{i}\right)=1-\sum_{i=1}^{m} p_{i}^{2} =1-\sum_{i=1}^{m}\left(\frac{\left|C_{k}\right|}{|D|}\right)^{2} \end{aligned}
\end{equation}
为啥说基尼系数计算速度快呢，因为基尼系数实际上是信息熵的一阶进似，作用等价于信息熵，只不过是简化版本。根据泰勒级数公式，将 $f(x)=-ln(x)$  在$x=1$处展开，忽略高阶无穷小，其可以等价为 $f(x)=1-x$,所以可以很容易得到上述定义。&lt;/p&gt;

&lt;h2 id=&quot;总结&quot;&gt;总结&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;$\underline{自信息}$是衡量随机变量中的某个事件发生时所带来的信息量的多少，越是不可能发生的事情发生了，那么自信息就越大；&lt;/li&gt;
  &lt;li&gt;$\underline{信息熵}$是衡量随机变量分布的混乱程度，是随机分布各事件发生的自信息的期望值，随机分布越宽广，则熵越大，越混乱；&lt;/li&gt;
  &lt;li&gt;信息熵推广到多维领域，则可得到$\underline{联合信息熵}$；&lt;/li&gt;
  &lt;li&gt;在某些先验条件下，自然引出$\underline{条件熵}$，其表示在X给定条件下，Y的条件概率分布熵对X的数学期望，没有啥特别的含义，是一个非常自然的概念；&lt;/li&gt;
  &lt;li&gt;前面的熵都是针对一个随机变量的，而$\underline{交叉熵、相对熵和互信息可以衡量两个随机变量之间的关系}$，三者作用几乎相同，只是应用范围和领域不同。
    &lt;ul&gt;
      &lt;li&gt;$\underline{交叉熵}$一般用在神经网络和逻辑回归中作为损失函数，&lt;/li&gt;
      &lt;li&gt;$\underline{相对熵}$一般用在生成模型中用于评估生成的分布和真实分布的差距，&lt;/li&gt;
      &lt;li&gt;而$\underline{互信息}$是纯数学的概念，作为一种评估两个分布之间相似性的数学工具，&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;其三者的关系是：
    &lt;ul&gt;
      &lt;li&gt;最大化似然函数，等价于最小化负对数似然，等价于最小化交叉熵，等价于最小化KL散度，互信息相对于相对熵区别就是互信息满足对称性；&lt;/li&gt;
      &lt;li&gt;作为熵的典型机器学习算法-决策树，广泛应用了熵进行特征划分，常用的有信息增益、信息增益率和基尼系数。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Fri, 21 Jun 2019 00:00:00 +0800</pubDate>
        <link>http://localhost:4000//2019/06/information-entropy.html</link>
        <guid isPermaLink="true">http://localhost:4000//2019/06/information-entropy.html</guid>
        
        <category>熵</category>
        
        
      </item>
    
      <item>
        <title>信息流产品的若干事项</title>
        <description>&lt;p&gt;互联网产品的品质可以归入两个点：黏性和规模。黏性是产品的内驱活力，好的产品，一定是有用户黏性的。这个表现为，能帮用户解决问题，满足用户的需要，被用户记住，并且在需要时想起。比如，相互介绍之后，就会想起，加个微信吧。微信提供了社交功能，并在这个场景中，被用户主动想起。规模是另一个关键的维度，这个能给产品不断地输入动力，并驱动着产品升级。没有内驱力的产品，是很难走远的，即使是获得了规模。虽然规模会直接表现为市场、收益和回报。&lt;/p&gt;

&lt;p&gt;如果规模的上升，不能提升产品黏性的提升，那是非常危险的。规模扩大的成本，相对较低。&lt;/p&gt;

</description>
        <pubDate>Sun, 16 Jun 2019 00:00:00 +0800</pubDate>
        <link>http://localhost:4000//2019/06/Ideas-about-a-good-product-for-internet.html</link>
        <guid isPermaLink="true">http://localhost:4000//2019/06/Ideas-about-a-good-product-for-internet.html</guid>
        
        <category>feed</category>
        
        <category>产品</category>
        
        <category>用户</category>
        
        
      </item>
    
      <item>
        <title>梦回山大</title>
        <description>&lt;p&gt;恍惚间，大学毕业5年了。5年了。这个假期宅在家，哪儿也没去，啥事也没做。不知怎的，我开始老想起大学里的事情，大一的兴隆山。翻看着人人网存下的照片，看着砖黄色的教学楼、实验室、宿舍楼，彷佛昨日，置身其中。沿着那条笔直笔直的玉兰大道，走过食堂，实验楼，是阶梯大教室，那些个座椅，那些连成片的教学楼；实验楼和教学楼之间有一个小广场，隔着一个瀑布，对面是图书馆，远处是工训中心；那个瀑布好像还有水，旁边是天工湖，还有先志大道。&lt;/p&gt;

&lt;p&gt;大一的画面的画面里，天刚下过雪，晚上，有路灯，我背着图板，走在去工图教室的路上。有点冷，有点冷清。我不知道，我为什么总是想起这个画面。好多次。可能，这个场景对我有着特别的意义吧。是孤独又骄傲的夜行，还是离群的落魄。不再去想。翻看着那些年同学的合影，他们笑得是那么的开心。于是，我开始渐渐明白，那时候自己的高傲和狭隘。那些年，所以相信的伟大，亦不过是虚幻的故事，那些励志的鸡汤，也不过是骗人的把戏。而我那些特立独行的坚持，自然有一定的意义。却也无端的失去了，那些本可以有的感动，友谊，认同。从未认真的走进别人的心里，也从未让别人真正的走进来。望着那一张张合影，独独没有我。不禁怅然。&lt;/p&gt;

&lt;p&gt;大一是懵懂的。而我还很偏执和胆小。结果就是逃避，和自我感动。捧着厚厚的理论物理，翻了一些页，做了一些自我陶醉式的满足。后面，得瑟地去转数学专业，哈工大交流，再到后来的尼山学堂，再到考清华的研究生。我似乎，只是为了博一个眼球，和自我陶醉。如果，那时候，我能沉下心，认真地想一想，这些选择需要的条件，和意味着什么。用心的考虑好，用心的去做，这才是真正需要的吧。&lt;/p&gt;

&lt;p&gt;诚然，在哈工大，我收获了很多。但也因此，失掉了整个的大学，整个的大学七零八落，从未有过认真的面对自己的内心。刻意和偏激。太用力的努力和坚持，心高气傲有些飘。飘了很多年。那些所谓的，看不上周围人，不过是一厢情愿的骗自己罢了。事实上，每个环境都是如此，存在即是合理。进一步说，如果真的给我不设限制的选择，其实也没有想去的。既然，来到了这个环境，至少是有匹配的点的。当然，我可以看得更远一些，但更重要的是，真正的沉淀和厚实的成长。用心，用力，但不要过。那样就是偏激了。&lt;/p&gt;

&lt;p&gt;也许，我是比那个环境和平台，要高一点。就像高中里，总是做好学生；在实验室，作出一点东西。但是，这并没有形成质的差距，所以这不是骄傲的理由。相反的，那些微弱的优势，如果不注意修炼，反而会滋长堕落的借口，固步自封，停滞不前。行百里者半九十。&lt;/p&gt;

&lt;p&gt;哈工大交流回来后，那些的吹嘘和傲娇，也只是虚荣。扪心自问，在哈工大，过得怎样？那里有大学的归属感么，别忘了在交换生最后一段时间里，对山大的思念。当然，这又陷入另一个问题，不要连续的抱怨。是的，我是山大的，山大是我的大学，哈工大是我做交换生的地方。两个大学，都是值得我尊重和细细体会的地方。&lt;/p&gt;

&lt;p&gt;毕业多年。那些年纠结的，归属感、认同感渐渐有了答案。那是我青春驻扎过的地方，那是花一样的岁月。归来，我重新做回少年，用心设计未来。&lt;/p&gt;

&lt;p&gt;稳住，少年；沉住气，小伙子。&lt;/p&gt;

</description>
        <pubDate>Sat, 04 May 2019 00:00:00 +0800</pubDate>
        <link>http://localhost:4000//2019/05/these-years.html</link>
        <guid isPermaLink="true">http://localhost:4000//2019/05/these-years.html</guid>
        
        <category>Think and Want</category>
        
        
      </item>
    
      <item>
        <title>linux learning</title>
        <description>&lt;ol&gt;
  &lt;li&gt;
    &lt;font color=&quot;Blue&quot;&gt;批量删除src_path下：&lt;/font&gt;
    &lt;ul&gt;
      &lt;li&gt;10天以前的文件：&lt;code class=&quot;highlighter-rouge&quot;&gt;find src_path -type f -mtime +20 -exec rm {} \;&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;20分钟以前的文件夹: &lt;code class=&quot;highlighter-rouge&quot;&gt;find src_path -type d -mmin +20 -exec rm {} \;&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;font color=&quot;Blue&quot;&gt;批量查找src_path下文件的包含to_find_context内容，例如：&lt;/font&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;grep -rn &quot;to_find_context&quot; src_path&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;font color=&quot;Blue&quot;&gt;在src_path 目录下，批量查找from_context并替换文件内容为 to_context：&lt;/font&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sed -i &quot;s/from_context/to_context/g&quot; `grep -rl &quot;from_context&quot; src_path`&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Tue, 12 Feb 2019 00:00:00 +0800</pubDate>
        <link>http://localhost:4000//2019/02/learn-linux-commands.html</link>
        <guid isPermaLink="true">http://localhost:4000//2019/02/learn-linux-commands.html</guid>
        
        <category>linux</category>
        
        <category>shell</category>
        
        
      </item>
    
      <item>
        <title>word2vec 源码解析</title>
        <description>&lt;h3 id=&quot;基本概念&quot;&gt;基本概念&lt;/h3&gt;
&lt;h4 id=&quot;在wrod2vec工具中有如下的几个比较重要的概念&quot;&gt;在wrod2vec工具中，有如下的几个比较重要的概念：&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;CBOW&lt;/li&gt;
  &lt;li&gt;Skip-Gram&lt;/li&gt;
  &lt;li&gt;Hierarchical Softmax&lt;/li&gt;
  &lt;li&gt;Negative Sampling&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;其中CBOW和Skip-Gram是word2vec工具中使用到的两种不同的语言模型，而Hierarchical Softmax和Negative Sampling是对以上的两种模型的具体的优化方法。&lt;/p&gt;

&lt;h4 id=&quot;在word2vec工具中主要的工作包括&quot;&gt;在word2vec工具中，主要的工作包括：&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;预处理。即变量的声明，全局变量的定义等；&lt;/li&gt;
  &lt;li&gt;构建词库。即包含文本的处理，以及是否需要有指定词库等；&lt;/li&gt;
  &lt;li&gt;初始化网络结构。即包含CBOW模型和Skip-Gram模型的参数初始化，Huffman编码的生成等；&lt;/li&gt;
  &lt;li&gt;多线程模型训练。即利用Hierarchical Softmax 或者 Negative Sampling 方法对网络中的参数进行求解；&lt;/li&gt;
  &lt;li&gt;最终结果的处理。即是否保存和以何种形式保存。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;对于以上的过程，可以由下图表示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/word2vec/code_seq.png&quot; alt=&quot;word2vec 代码实现流程&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;1预处理&quot;&gt;1、预处理&lt;/h3&gt;
&lt;p&gt;在预处理部分，对word2vec需要使用的参数进行初始化，在word2vec中是利用传入的方式对参数进行初始化的。&lt;/p&gt;

&lt;p&gt;在预处理部分，实现了sigmoid函数值的近似计算。在利用神经网络模型对样本进行预测的过程中，需要对其进行预测，此时，需要使用到sigmoid函数，sigmoid函数的具体形式为：&lt;/p&gt;

&lt;p&gt;\begin{equation}
\sigma = \frac{1}{1+ e^{-x}} = \frac{e^{-x}}{1 + e^{-x}}
\end{equation}&lt;/p&gt;

&lt;p&gt;如果每一次都请求计算&lt;code class=&quot;highlighter-rouge&quot;&gt;sigmoid&lt;/code&gt;值，对性能将会有一定的影响，当&lt;code class=&quot;highlighter-rouge&quot;&gt;sigmoid&lt;/code&gt;的值对精度的要求并不是非常严格时，可以采用近似计算。在&lt;code class=&quot;highlighter-rouge&quot;&gt;word2vec&lt;/code&gt;中，将区间&lt;code class=&quot;highlighter-rouge&quot;&gt;[−6,6]&lt;/code&gt;（设置的参数&lt;code class=&quot;highlighter-rouge&quot;&gt;MAX_EXP&lt;/code&gt;为6）等距离划分成&lt;code class=&quot;highlighter-rouge&quot;&gt;EXP_TABLE_SIZE&lt;/code&gt;等份，并将每个区间中的&lt;code class=&quot;highlighter-rouge&quot;&gt;sigmoid&lt;/code&gt;值计算好存入到数组&lt;code class=&quot;highlighter-rouge&quot;&gt;expTable&lt;/code&gt;中，需要使用时，直接从数组中查找。计算&lt;code class=&quot;highlighter-rouge&quot;&gt;sigmoid&lt;/code&gt;值的代码如下所示：&lt;/p&gt;

&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;expTable&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;real&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;malloc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;EXP_TABLE_SIZE&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;sizeof&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;real&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;// 申请EXP_TABLE_SIZE+1个空间&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;// 计算sigmoid值&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;EXP_TABLE_SIZE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;expTable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;real&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;EXP_TABLE_SIZE&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MAX_EXP&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// Precompute the exp() table&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;expTable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;expTable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;expTable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;                   &lt;span class=&quot;c1&quot;&gt;// Precompute f(x) = x / (x + 1)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;注意：在上述代码中，作者使用的是小于EXP_TABLE_SIZE，实际的区间是&lt;code class=&quot;highlighter-rouge&quot;&gt;[−6,6)&lt;/code&gt;。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;2构建词库&quot;&gt;2、构建词库&lt;/h3&gt;
&lt;p&gt;在word2vec源码中，提供了两种构建词库的方法，分别为：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;指定词库：ReadVocab()方法&lt;/li&gt;
  &lt;li&gt;从词的文本构建词库：LearnVocabFromTrainFile()方法&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;21构建词库的过程&quot;&gt;2.1、构建词库的过程&lt;/h4&gt;
&lt;p&gt;在这里，我们以从词的文本构建词库为例。构建词库的过程如下所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/word2vec/vocab.png&quot; alt=&quot;构建词库&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在这部分中，最主要的工作是对文本进行处理，包括低频词的处理，hash表的处理等等。首先，会在词库中增加一个“&amp;lt; /s&amp;gt;”的词，同时，在读取文本的过程中，将换行符“\n”也表示成该该词，如：&lt;/p&gt;
&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;sc&quot;&gt;'\n'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;strcpy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;word&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&amp;lt;/s&amp;gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;// 换行符用&amp;lt;/s&amp;gt;表示&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;在循环的过程中，不断去读取文件中的每一个词，并在词库中进行查找，若存在该词，则该词的词频+1，否则，在词库中增加该词。在词库中，是通过哈希表的形式存储的。最终，会过滤掉一些低频词。&lt;/p&gt;

&lt;p&gt;在得到最终的词库之前，还需根据词库中的词频(由高到低)对词库中的词进行排序。&lt;/p&gt;

&lt;h4 id=&quot;22对词的哈希处理&quot;&gt;2.2、对词的哈希处理&lt;/h4&gt;
&lt;p&gt;在存储词的过程中，同时保留这两个数组：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;存储词的vocab&lt;/li&gt;
  &lt;li&gt;存储词的hash的vocab_hash
其中，在vocab中，存储的是词对应的结构体：
    &lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;// 词的结构体&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;struct&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vocab_word&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// 出现的次数&lt;/span&gt;
      &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;point&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// 从根结点到叶子节点的路径&lt;/span&gt;
      &lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;word&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;code&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;codelen&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;// 分别对应着词，Huffman编码，编码长度&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;p&gt;在vocab_hash中存储的值是词在词库中的Index。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在对词的处理过程中，主要包括：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;计算词的hash值：
    &lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;// 取词的hash值&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;GetWordHash&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;word&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;kt&quot;&gt;unsigned&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hash&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;strlen&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;word&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hash&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hash&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;257&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;word&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;hash&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hash&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vocab_hash_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hash&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;检索词是否存在。如不存在则返回-1，否则，返回该词在词库中的索引：
    &lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab_hash&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hash&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;// 不存在该词&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strcmp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;word&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab_hash&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hash&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;word&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vocab_hash&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hash&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;// 返回索引值&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;hash&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hash&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vocab_hash_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;// 处理冲突&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;// 不存在该词&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;p&gt;在这个过程中，使用到了线性探测的开放定址法处理冲突，开放定址法就是一旦发生冲突，就去寻找下一个空的散列地址。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;不存在，则插入新词。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在这个过程中，除了需要将词增加到词库中，好需要计算该词的hash值，并将vocab_hash数组中的值标记为索引。&lt;/p&gt;

&lt;h4 id=&quot;23对低频词的处理&quot;&gt;2.3、对低频词的处理&lt;/h4&gt;
&lt;p&gt;在循环读取每一个词的过程中，当出现“vocab_size &amp;gt; vocab_hash_size * 0.7”时，需要对低频词进行处理。其中，vocab_size表示的是目前词库中词的个数，vocab_hash_size表示的是初始设定的hash表的大小。&lt;/p&gt;

&lt;p&gt;在处理低频词的过程中，通过参数“min_reduce”来控制，若词出现的次数小于等于该值时，则从词库中删除该词。&lt;/p&gt;

&lt;p&gt;在删除了低频词后，需要重新对词库中的词进行hash值的计算。&lt;/p&gt;

&lt;h4 id=&quot;24根据词频对词库中的词排序&quot;&gt;2.4、根据词频对词库中的词排序&lt;/h4&gt;
&lt;p&gt;基于以上的过程，程序已经将词从文件中提取出来，并存入到指定的词库中（vocab数组），接下来，需要根据每一个词的词频对词库中的词按照词频从大到小排序，其基本过程在函数SortVocab中，排序过程为：&lt;/p&gt;

&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;qsort&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vocab_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;sizeof&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;struct&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vocab_word&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;VocabCompare&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;保持字符“&amp;lt; \s&amp;gt;”在最开始的位置。排序后，根据“min_count”对低频词进行处理，与上述一样，再对剩下的词重新计算hash值。&lt;/p&gt;

&lt;p&gt;至此，整个对词的处理过程就已经结束了。接下来，将是对网络结构的处理和词向量的训练。&lt;/p&gt;

&lt;h3 id=&quot;3初始化网络结构&quot;&gt;3、初始化网络结构&lt;/h3&gt;

&lt;p&gt;有了以上的对词的处理，就已经处理好了所有的训练样本，此时，便可以开始网络结构的初始化和接下来的网络训练。网络的初始化的过程在InitNet()函数中完成。&lt;/p&gt;

&lt;h4 id=&quot;31初始化网络参数&quot;&gt;3.1、初始化网络参数&lt;/h4&gt;
&lt;p&gt;在初始化的过程中，主要的参数包括词向量的初始化和映射层到输出层的权重的初始化，如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/word2vec/net.png&quot; alt=&quot;网络参数&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在初始化的过程中，映射层到输出层的权重都初始化为0.0，而对于每一个词向量的初始化，作者的初始化方法如下代码所示：&lt;/p&gt;
&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vocab_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layer1_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;next_random&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;next_random&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;unsigned&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;25214903917&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;// 1、与：相当于将数控制在一定范围内&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;// 2、0xFFFF：65536&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;// 3、/65536：[0,1]之间&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;syn0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layer1_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;next_random&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;mh&quot;&gt;0xFFFF&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;real&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;65536&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layer1_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;// 初始化词向量&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;首先，生成一个很大的&lt;code class=&quot;highlighter-rouge&quot;&gt;next_random&lt;/code&gt;的数，通过与“0xFFFF”进行与运算截断，再除以65536得到&lt;code class=&quot;highlighter-rouge&quot;&gt;[0,1]&lt;/code&gt;之间的数，最终，得到的初始化的向量的范围为：&lt;code class=&quot;highlighter-rouge&quot;&gt;[−0.5m,0.5m]&lt;/code&gt;，其中，mm为词向量的长度。&lt;/p&gt;

&lt;h4 id=&quot;32huffman树的构建&quot;&gt;3.2、Huffman树的构建&lt;/h4&gt;
&lt;p&gt;在层次Softmax中需要使用到Huffman树以及Huffman编码，因此，在网络结构的初始化过程中，也需要初始化Huffman树。在生成Huffman树的过程中，首先定义了33个长度为vocab_size*2+1的数组：&lt;/p&gt;
&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;count&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;calloc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;sizeof&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;binary&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;calloc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;sizeof&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parent_node&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;calloc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;sizeof&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;其中，count数组中前vocab_size存储的是每一个词的对应的词频，后面初始化的是很大的数，已知词库中的词是按照降序排列的，因此，构建Huffman树的过程如下所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/word2vec/huff_1.png&quot; alt=&quot;huffman 数据结构&quot; /&gt;&lt;/p&gt;

&lt;p&gt;首先，设置两个指针pos1和pos2，分别指向最后一个词和最后一个词的后一位，从两个指针所指的数中选择出最小的值，记为min1i，如pos1所指的值最小，此时，将pos1左移，再比较pos1和pos2所指的数，选择出最小的值，记为min2i，将他们的和存储到pos2所指的位置。并将此时pos2所指的位置设置为min1i和min2i的父节点，同时，记min2i所指的位置的编码为1，如下代码所示：&lt;/p&gt;

&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;// 设置父节点&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;parent_node&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;min1i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vocab_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;parent_node&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;min2i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vocab_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;binary&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;min2i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;// 设置一个子树的编码为1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;构建好Huffman树后，此时，需要根据构建好的Huffman树生成对应节点的Huffman编码。假设，上述的数据生成的最终的Huffman树为：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/word2vec/huff_2.png&quot; alt=&quot;huffman树&quot; /&gt;&lt;/p&gt;

&lt;p&gt;此时，count数组，binary数组和parent_node数组分别为：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/word2vec/huff_3.png&quot; alt=&quot;数组值&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在生成Huffman编码的过程中，针对每一个词（词都在叶子节点上），从叶子节点开始，将编码存入到code数组中，如对于上图中的“R”节点来说，其code数组为{1,0}，再对其反转便是Huffman编码：&lt;/p&gt;

&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;codelen&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;// 词的编码长度&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;point&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vocab_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;code&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;code&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;// 编码的反转&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;point&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;point&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vocab_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;// 记录的是从根结点到叶子节点的路径&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;注意：这里的Huffman树的构建和Huffman编码的生成过程写得比较精简。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;33负样本选中表的初始化&quot;&gt;3.3、负样本选中表的初始化&lt;/h4&gt;
&lt;p&gt;如果是采用负采样的方法，此时还需要初始化每个词被选中的概率。在所有的词构成的词典中，每一个词出现的频率有高有低，我们希望，对于那些高频的词，被选中成为负样本的概率要大点，同时，对于那些出现频率比较低的词，我们希望其被选中成为负样本的频率低点。这个原理于“轮盘赌”的策略一致（详细可以参见“优化算法——遗传算法”）。在程序中，实现这部分功能的代码为：&lt;/p&gt;

&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;// 生成负采样的概率表&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;InitUnigramTable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_words_pow&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;power&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.75&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;table&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;malloc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;table_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;sizeof&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;// int --&amp;gt; int&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vocab_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_words_pow&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;power&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;// 类似轮盘赌生成每个词的概率&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;d1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;power&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_words_pow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;table_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;table&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;double&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;table_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                        &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
                        &lt;span class=&quot;n&quot;&gt;d1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;power&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_words_pow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
                &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vocab_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vocab_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;在实现的过程中，没有直接使用每一个词的频率，而是使用了词的0.75次方。&lt;/p&gt;

&lt;h3 id=&quot;4多线程模型训练&quot;&gt;4、多线程模型训练&lt;/h3&gt;
&lt;p&gt;以上的各个部分是为训练词向量做准备，即准备训练数据，构建训练模型。在上述的初始化完成后，接下来就是根据不同的方法对模型进行训练，在实现的过程中，作者使用了多线程的方法对其进行训练。&lt;/p&gt;

&lt;h4 id=&quot;41多线程的处理&quot;&gt;4.1、多线程的处理&lt;/h4&gt;
&lt;p&gt;为了能够对文本进行加速训练，在实现的过程中，作者使用了多线程的方法，并对每一个线程上分配指定大小的文件：&lt;/p&gt;
&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;// 利用多线程对训练文件划分，每个线程训练一部分的数据&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;fseek&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;file_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_threads&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SEEK_SET&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;注意：这边的多线程分割方式并不能保证每一个线程分到的文件是互斥的。对于其中的原因，可以参见“Linux C 编程——多线程”。&lt;/p&gt;

&lt;p&gt;这个过程可以通过下图简单的描述：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/word2vec/pthread.png&quot; alt=&quot;多线程处理&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在实现多线程的过程中，作者并没有加锁的操作，而是对模型参数和词向量的修改可以任意执行，这一点类似于基于随机梯度的方法，训练的过程与训练样本的训练是没有关系的，这样可以大大加快对词向量的训练。抛开多线程的部分，在每一个线程内执行的是对模型和词向量的训练。&lt;/p&gt;

&lt;p&gt;作者在实现的过程中，主要实现了两个模型，即CBOW模型和Skip-gram模型，在每个模型中，又分别使用到了两种不同的训练方法，即层次Softmax和Negative Sampling方法。&lt;/p&gt;

&lt;p&gt;对于CBOW模型和Skip-gram模型的理解，首先必须知道统计语言模型（Statistic Language Model）。&lt;/p&gt;

&lt;p&gt;在统计语言模型中的核心内容是：$\underline{计算一组词语能够成为一个句子的概率}$。&lt;/p&gt;

&lt;p&gt;为了能够求解其中的参数，一大批参数求解的方法被提出，在其中，就有word2vec中要使用的神经概率语言模型。具体的神经概率语言模型可以参见“”。&lt;/p&gt;

&lt;h4 id=&quot;42cbow模型&quot;&gt;4.2、CBOW模型&lt;/h4&gt;

&lt;p&gt;CBOW模型和Skip-gram模型是神经概率语言模型的两种变形形式，其中，在CBOW模型中包含三层，即输入层，映射层和输出层。对于CBOW模型，如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/word2vec/cbow.png&quot; alt=&quot;cbow模型&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在CBOW模型中，通过词$w_t$的前后词$w_{t-2},w_{t-1},w_{t+1},w_{t+2}$来预测当前词$w_t$。此处的窗口的大小 $window = 2$。&lt;/p&gt;

&lt;h4 id=&quot;43-参数更新过程&quot;&gt;4.3 参数更新过程&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;前向的处理 + 反向损失梯度传播&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;431从输入层到映射层&quot;&gt;4.3.1、从输入层到映射层&lt;/h5&gt;
&lt;p&gt;首先找到每个词对应的词向量，并将这些词的词向量相加，程序代码如下所示：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;// in -&amp;gt; hidden
// 输入层到映射层
cw = 0;
for (a = b; a &amp;lt; window * 2 + 1 - b; a++) if (a != window) {
    c = sentence_position - window + a;// sentence_position表示的是当前的位置
    // 判断c是否越界
    if (c &amp;lt; 0) continue;
    if (c &amp;gt;= sentence_length) continue;

    last_word = sen[c];// 找到c对应的索引
    if (last_word == -1) continue;

    for (c = 0; c &amp;lt; layer1_size; c++) neu1[c] += syn0[c + last_word * layer1_size];// 累加
    cw++;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;当累加完窗口内的所有的词向量的之后，存储在映射层neu1中，并取平均，程序代码如下所示：&lt;/p&gt;
&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layer1_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;neu1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cw&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;// 计算均值&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;当取得了映射层的结果后，此时就需要使用Hierarchical Softmax或者Negative Sampling对模型进行训练。&lt;/p&gt;

&lt;h5 id=&quot;432hierarchical-softmax&quot;&gt;4.3.2、Hierarchical Softmax&lt;/h5&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/word2vec/hs.png&quot; alt=&quot;hs模型的数学原理&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;word&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;codelen&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;// word为当前词&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;// 计算输出层的输出&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;l2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;word&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;point&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layer1_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;// 找到第d个词对应的权重&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;// Propagate hidden -&amp;gt; output&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layer1_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;neu1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;syn1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;// 映射层到输出层&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MAX_EXP&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;continue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MAX_EXP&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;continue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;expTable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MAX_EXP&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;EXP_TABLE_SIZE&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MAX_EXP&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))];&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;// Sigmoid结果&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;// 'g' is the gradient multiplied by the learning rate&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;word&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;code&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;// Propagate errors output -&amp;gt; hidden&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layer1_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;neu1e&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;syn1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;// 修改映射后的结果&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;// Learn weights hidden -&amp;gt; output&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layer1_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;syn1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;neu1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;// 修改映射层到输出层之间的权重&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;对于窗口内的词的向量的更新，则是利用窗口内的所有词的梯度之和$neu1e = \sum{\frac{\partial{L(w,j)}}{\partial{X_w}}}$ 来更新，如程序代码所示：&lt;/p&gt;

&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;// hidden -&amp;gt; in&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;// 以上是从映射层到输出层的修改，现在返回修改每一个词向量&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;window&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;window&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sentence_position&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;window&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;continue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sentence_length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;continue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;last_word&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sen&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;last_word&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;continue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;// 利用窗口内的所有词向量的梯度之和来更新&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layer1_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;syn0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;last_word&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layer1_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;neu1e&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h5 id=&quot;433negative-sampling&quot;&gt;4.3.3、Negative Sampling&lt;/h5&gt;
&lt;p&gt;与Hierarchical Softmax一致，Negative Sampling也是一种加速计算的方法，在Negative Sampling方法中使用的是随机的负采样，在CBOW模型中，已知词$w$的上下文$context(w)$，需要预测词$w$，对于给定的上下文$context(w)$，词$w$即为正样本，其他的样本为负样本，此时我们需要根据词频从剩下的词中挑选出最合适的负样本，实现的代码如下所示：&lt;/p&gt;
&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;// 标记target和label&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;// 正样本&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;word&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;// 选择出负样本&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;next_random&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;next_random&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;unsigned&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;25214903917&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;table&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;next_random&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;table_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;// 从table表中选择出负样本&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;// 重新选择&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;next_random&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;word&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;continue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/word2vec/neg.png&quot; alt=&quot;neg模型的数学原理&quot; /&gt;&lt;/p&gt;

&lt;p&gt;代码更新为：&lt;/p&gt;
&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MAX_EXP&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;label&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MAX_EXP&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;label&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;label&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;expTable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MAX_EXP&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;EXP_TABLE_SIZE&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MAX_EXP&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layer1_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;neu1e&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;syn1neg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layer1_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;syn1neg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;neu1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;对词向量的更新与Hierarchical Softmax中一致。&lt;/p&gt;

&lt;h4 id=&quot;43skip-gram模型&quot;&gt;4.3、Skip-gram模型&lt;/h4&gt;

&lt;p&gt;而Skip-gram模型与CBOW正好相反，在Skip-gram模型中，则是通过当前词$w_t$来预测其前后词$w_{t-2},w_{t-1},w_{t+1},w_{t+2}$，Skip-gram模型如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/word2vec/skg.png&quot; alt=&quot;skip-gram 模型&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;431hierarchical-softmax&quot;&gt;4.3.1、Hierarchical Softmax&lt;/h5&gt;

&lt;p&gt;由上述的分析，我们发现，在Skip-gram模型中，其计算方法与CBOW模型很相似，不同的是，在Skip-gram模型中，需要使用当前词分别预测窗口中的词，因此，这是一个循环的过程：&lt;/p&gt;
&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;window&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;window&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;对于向量的更新过程与CBOW模型中的Hierarchical Softmax一致：&lt;/p&gt;

&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sentence_position&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;window&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;continue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sentence_length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;continue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;last_word&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sen&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;last_word&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;continue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;l1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;last_word&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layer1_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layer1_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;neu1e&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;// HIERARCHICAL SOFTMAX&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;word&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;codelen&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;l2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;word&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;point&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layer1_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;// Propagate hidden -&amp;gt; output&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;// 映射层即为输入层&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layer1_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;syn0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;syn1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MAX_EXP&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;continue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MAX_EXP&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;continue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;expTable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MAX_EXP&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;EXP_TABLE_SIZE&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MAX_EXP&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))];&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;// 'g' is the gradient multiplied by the learning rate&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;word&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;code&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;// Propagate errors output -&amp;gt; hidden&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layer1_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;neu1e&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;syn1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;// Learn weights hidden -&amp;gt; output&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layer1_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;syn1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;syn0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;// Learn weights input -&amp;gt; hidden&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layer1_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;syn0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;neu1e&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h5 id=&quot;432negative-sampling&quot;&gt;4.3.2、Negative Sampling&lt;/h5&gt;
&lt;p&gt;与上述一致，在Skip-gram中与CBOW中的唯一不同是在Skip-gram中是循环的过程。代码的实现类似与上面的Hierarchical Softmax。&lt;/p&gt;

&lt;p&gt;注释版的word2vec源码已经上传到Github中：Github：&lt;a href=&quot;https://github.com/zhaozhiyong19890102/OpenSourceReading/blob/master/word2vec/word2vec.c&quot;&gt;word2vec.c&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;作者：zhiyong_will 
来源：CSDN 
原文：https://blog.csdn.net/google19890102/article/details/51887344 
版权声明：本文为博主原创文章，转载请附上博文链接！
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
</description>
        <pubDate>Wed, 06 Feb 2019 00:00:00 +0800</pubDate>
        <link>http://localhost:4000//2019/02/word-2-vec-code-review.html</link>
        <guid isPermaLink="true">http://localhost:4000//2019/02/word-2-vec-code-review.html</guid>
        
        <category>word2vec</category>
        
        <category>原理</category>
        
        <category>源码</category>
        
        
      </item>
    
  </channel>
</rss>
